{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨å±€å¼€å…³ï¼šTrue è¡¨ç¤ºå¯ç”¨è‡ªåŠ¨è¿½åŠ åˆ°æ–‡ä»¶ï¼ŒFalse è¡¨ç¤ºå…³é—­\n",
    "write_to_file = False  # åˆå§‹é»˜è®¤å…³é—­ï¼Œéœ€è¦æ—¶æ‰‹åŠ¨æ”¹ä¸º True\n",
    "\n",
    "# ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆå›ºå®šä¸º Assign1-1.pyï¼‰\n",
    "target_file = \"Assign1-1.py\"\n",
    "\n",
    "def auto_write(cell_content):\n",
    "    \"\"\"\n",
    "    è‡ªåŠ¨æ‰§è¡Œä»£ç å¹¶æ ¹æ®å¼€å…³å†³å®šæ˜¯å¦è¿½åŠ åˆ°æ–‡ä»¶\n",
    "    å‚æ•°ï¼šcell_content (str)ï¼šå½“å‰ä»£ç å—çš„å†…å®¹ï¼ˆç”¨ä¸‰å¼•å·åŒ…è£¹ï¼‰\n",
    "    \"\"\"\n",
    "    # 1. å…ˆæ‰§è¡Œå½“å‰ä»£ç ï¼ˆç¡®ä¿åŠŸèƒ½æ­£å¸¸ï¼‰\n",
    "    exec(cell_content, globals())\n",
    "    \n",
    "    # 2. å¦‚æœå¼€å…³å¼€å¯ï¼Œè¿½åŠ åˆ°ç›®æ ‡æ–‡ä»¶\n",
    "    if write_to_file:\n",
    "        from IPython import get_ipython\n",
    "        get_ipython().run_cell_magic('file', f'-a {target_file}', cell_content)\n",
    "        print(f\"å·²è¿½åŠ åˆ° {target_file}\")\n",
    "    else:\n",
    "        print(\"æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22b6ac",
   "metadata": {},
   "source": [
    "# å®‰è£…ä¾èµ–åŒ…å’Œå¼•ç”¨åŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4cd6605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "auto_write(\"\"\"\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4abd4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write(\"\"\" \n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import contractions\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "from opencc import OpenCC\n",
    "import jieba\n",
    "cc_zh = OpenCC('t2s')  # ä¸­æ–‡ç¹ä½“è½¬ç®€ä½“\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bc1016a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write(\"\"\"\n",
    "from tqdm import tqdm  # å¯¼å…¥tqdmåº“\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path  # ç”¨äºè·¯å¾„éªŒè¯\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8fe71",
   "metadata": {},
   "source": [
    "# æ£€æµ‹è¯­è¨€ç±»å‹ (ä»…æ”¯æŒä¸­æ–‡ã€è‹±æ–‡)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ac7ea0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write(\"\"\"\n",
    "def detect_language(text):\n",
    "    '''\n",
    "    æ›´ä¸¥è°¨çš„å®ç°æ˜¯ä½¿ç”¨langdetectåŒ…ï¼Œä½†è¯¥åŒ…æœ‰æ—¶ä¼šè¯¯åˆ¤ä¸­è‹±æ··åˆæ–‡æœ¬ä¸ºå…¶ä»–è¯­è¨€\n",
    "    éœ€è¦å®‰è£…ï¼špip install langdetect\n",
    "    from langdetect import detect, LangDetectException\n",
    "\n",
    "    def detect_language(text):\n",
    "        if not text.strip():\n",
    "            return 'unknown'\n",
    "        try:\n",
    "            lang = detect(text)\n",
    "            # æ˜ å°„ä¸ºä¸­æ–‡ï¼ˆzhï¼‰æˆ–è‹±æ–‡ï¼ˆenï¼‰ï¼Œå…¶ä»–è¯­è¨€è¿”å›unknown\n",
    "            return 'zh' if lang == 'zh-cn' else 'en' if lang == 'en' else 'unknown'\n",
    "        except LangDetectException:\n",
    "            return 'unknown'\n",
    "    '''\n",
    "    if not text.strip():\n",
    "        return 'unknown'\n",
    "    \n",
    "    # æå–ä¸­æ–‡CJKå­—ç¬¦å’Œè‹±æ–‡å•è¯å­—ç¬¦\n",
    "    cjk_chars = re.findall(r'[\\\\u4e00-\\\\u9fff]', text)\n",
    "    en_chars = re.findall(r'[a-zA-Z]', text)  # è‹±æ–‡ç‰¹å¾ï¼šå­—æ¯\n",
    "    \n",
    "    total = len(text.strip())\n",
    "    if total == 0:\n",
    "        return 'unknown'\n",
    "    \n",
    "    cjk_ratio = len(cjk_chars) / total\n",
    "    en_ratio = len(en_chars) / total\n",
    "    \n",
    "    # ä¸­æ–‡åˆ¤å®šï¼šCJKå æ¯”è¶…30%\n",
    "    if cjk_ratio > 0.3:\n",
    "        return 'zh'\n",
    "    # è‹±æ–‡åˆ¤å®šï¼šè‹±æ–‡å æ¯”è¶…30%ï¼Œä¸”CJKå æ¯”æä½ï¼ˆé¿å…ä¸­è‹±æ··åˆæ–‡æœ¬è¯¯åˆ¤ï¼‰\n",
    "    elif en_ratio > 0.3 and cjk_ratio < 0.1:\n",
    "        return 'en'\n",
    "    # å…¶ä»–æƒ…å†µï¼ˆå¦‚ä¸­è‹±æ··åˆã€å…¶ä»–è¯­è¨€ï¼‰\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31a51267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write(\"\"\"\n",
    "def get_statistics(text, n=10, k=10, lang='en'):\n",
    "    '''\n",
    "    è·å–æ–‡æœ¬ä¸­å‰nä¸ªé«˜é¢‘è¯å’Œé•¿åº¦æœ€é•¿çš„kä¸ªè¯\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text: è¾“å…¥æ–‡æœ¬\n",
    "        n: é«˜é¢‘è¯æ•°é‡ï¼ˆé»˜è®¤10ï¼‰\n",
    "        k: æœ€é•¿è¯æ•°é‡ï¼ˆé»˜è®¤10ï¼‰\n",
    "        lang: è¯­è¨€ï¼ˆ'en'è‹±æ–‡/'zh'ä¸­æ–‡ï¼Œé»˜è®¤'en'ï¼‰\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        å…ƒç»„ (top_n_words, longest_k_words)ï¼Œå…¶ä¸­ï¼š\n",
    "            - top_n_words: åˆ—è¡¨ï¼Œå…ƒç´ ä¸º (è¯, é¢‘ç‡) å…ƒç»„ï¼ŒæŒ‰é¢‘ç‡é™åºæ’åˆ—\n",
    "            - longest_k_words: åˆ—è¡¨ï¼Œå…ƒç´ ä¸º (è¯, é•¿åº¦) å…ƒç»„ï¼ŒæŒ‰é•¿åº¦é™åºæ’åˆ—ï¼ˆé•¿åº¦ç›¸åŒåˆ™æŒ‰è¯æœ¬èº«æ’åºï¼‰\n",
    "    '''    \n",
    "    if not text.strip():\n",
    "        return ([], [])  # ç©ºæ–‡æœ¬è¿”å›ä¸¤ä¸ªç©ºåˆ—è¡¨\n",
    "    \n",
    "    # 1. åˆ†è¯ï¼ˆä¸­è‹±æ–‡é€‚é…ï¼‰\n",
    "    if lang == 'en':\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    elif lang == 'zh':\n",
    "        tokens = jieba.lcut(text)  # ä¸­æ–‡ç²¾ç¡®åˆ†è¯\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language. Use 'en' or 'zh'.\")\n",
    "    \n",
    "    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼ˆç§»é™¤çº¯ç©ºæ ¼/ç©ºçš„tokenï¼‰\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    tokens = [token for token in tokens if token]  # ä¿ç•™éç©ºtoken\n",
    "    \n",
    "    # 2. è®¡ç®—å‰nä¸ªé«˜é¢‘è¯ï¼ˆå¤ç”¨åŸé€»è¾‘ï¼‰\n",
    "    fdist = FreqDist(tokens)\n",
    "    top_n_words = fdist.most_common(n)  # æ ¼å¼ï¼š[(è¯1, é¢‘ç‡1), (è¯2, é¢‘ç‡2), ...]\n",
    "    \n",
    "    # 3. è®¡ç®—é•¿åº¦æœ€é•¿çš„kä¸ªè¯ï¼ˆå»é‡åæŒ‰é•¿åº¦æ’åºï¼‰\n",
    "    unique_tokens = list(set(tokens))  # å»é‡ï¼Œé¿å…é‡å¤è¯å æ®ä½ç½®\n",
    "    # æŒ‰é•¿åº¦é™åºæ’åºï¼ˆé•¿åº¦ç›¸åŒåˆ™æŒ‰è¯çš„å­—æ¯/å­—ç¬¦é¡ºåºæ’åºï¼Œä¿è¯ç¨³å®šæ€§ï¼‰\n",
    "    sorted_by_length = sorted(\n",
    "        unique_tokens,\n",
    "        key=lambda x: (-len(x), x)  # å…ˆæŒ‰é•¿åº¦çš„è´Ÿæ•°ï¼ˆé™åºï¼‰ï¼Œå†æŒ‰è¯æœ¬èº«ï¼ˆå‡åºï¼‰\n",
    "    )\n",
    "    # å–å‰kä¸ªï¼Œå¹¶æ·»åŠ é•¿åº¦ä¿¡æ¯\n",
    "    longest_k_words = [(word, len(word)) for word in sorted_by_length[:k]]\n",
    "    \n",
    "    return (top_n_words, longest_k_words)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ee9e4",
   "metadata": {},
   "source": [
    "# å…³é”®è¾“å‡ºå­å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a996b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write('''\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ–‡æœ¬ä¸­çš„HTMLæ ‡ç­¾å¹¶æå–çº¯æ–‡æœ¬å†…å®¹ï¼ŒåŒæ—¶æ ‡å‡†åŒ–æ¢è¡Œç¬¦ã€‚\n",
    "\n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "        1. ä½¿ç”¨BeautifulSoupè§£æHTMLæ–‡æœ¬ï¼Œç§»é™¤`<iframe>`å’Œ`<script>`æ ‡ç­¾ï¼ˆé€šå¸¸åŒ…å«éæ–‡æœ¬å†…å®¹ï¼‰\n",
    "        2. æå–HTMLä¸­çš„çº¯æ–‡æœ¬å†…å®¹\n",
    "        3. å°†å„ç§æ¢è¡Œç¬¦ï¼ˆ\\\\rã€\\\\nã€\\\\r\\\\nï¼‰ç»Ÿä¸€æ›¿æ¢ä¸ºå•ä¸ª\\\\nï¼Œç¡®ä¿æ¢è¡Œæ ¼å¼ä¸€è‡´\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šåŒ…å«HTMLæ ‡ç­¾çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        strï¼šç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾åçš„çº¯æ–‡æœ¬ï¼Œæ¢è¡Œç¬¦å·²æ ‡å‡†åŒ–ä¸º\\\\n\n",
    "\n",
    "    ä¾èµ–ï¼š\n",
    "        éœ€è¦å®‰è£…BeautifulSoupåº“ï¼ˆpip install beautifulsoup4ï¼‰\n",
    "\n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> raw_html = \"<p>Hello<br>World!</p><script>alert('test')</script>\"\n",
    "        >>> strip_html_tags(raw_html)\n",
    "        'Hello\\\\nWorld!'\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ–‡æœ¬ä¸­çš„é‡éŸ³å­—ç¬¦ï¼ˆå¦‚Ã©ã€Ã±ã€Ã¼ç­‰ï¼‰ï¼Œå°†å…¶è½¬æ¢ä¸ºæ— é‡éŸ³çš„åŸºç¡€å­—ç¬¦ã€‚\n",
    "    \n",
    "    å¤„ç†é€»è¾‘ï¼š\n",
    "        1. ä½¿ç”¨`unicodedata.normalize('NFKD', text)`å¯¹æ–‡æœ¬è¿›è¡ŒUnicodeè§„èŒƒåŒ–ï¼š\n",
    "           - NFKDï¼ˆCompatibility Decomposition, Canonical Compositionï¼‰ä¼šå°†é‡éŸ³å­—ç¬¦åˆ†è§£ä¸º\n",
    "             åŸºç¡€å­—ç¬¦ + é‡éŸ³ç¬¦å·ï¼ˆä¾‹å¦‚ï¼š'Ã©' åˆ†è§£ä¸º 'e' + é‡éŸ³ç¬¦å·ï¼‰ã€‚\n",
    "        2. é€šè¿‡`.encode('ascii', 'ignore')`å°†è§„èŒƒåŒ–åçš„æ–‡æœ¬ç¼–ç ä¸ºASCIIï¼š\n",
    "           - ASCIIç¼–ç ä¸æ”¯æŒé‡éŸ³ç¬¦å·ï¼Œ`ignore`å‚æ•°ä¼šå¿½ç•¥æ— æ³•ç¼–ç çš„é‡éŸ³ç¬¦å·éƒ¨åˆ†ï¼Œä»…ä¿ç•™åŸºç¡€å­—ç¬¦ã€‚\n",
    "        3. å†é€šè¿‡`.decode('utf-8', 'ignore')`å°†å­—èŠ‚æµè§£ç å›UTF-8å­—ç¬¦ä¸²ï¼Œå¾—åˆ°æ— é‡éŸ³çš„ç»“æœã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šåŒ…å«é‡éŸ³å­—ç¬¦çš„åŸå§‹æ–‡æœ¬\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        strï¼šç§»é™¤é‡éŸ³åçš„æ–‡æœ¬ï¼ˆä»…åŒ…å«ASCIIå¯è¡¨ç¤ºçš„å­—ç¬¦ï¼‰\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> remove_accented_chars(\"cafÃ© clichÃ© naÃ¯ve\")\n",
    "        'cafe cliche naive'\n",
    "        >>> remove_accented_chars(\"Ã Ã¨Ã¬Ã²Ã¹ ÃÃ‰ÃÃ“Ãš Ã± Ã§\")\n",
    "        'aeiou AEIOU n c'\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    \"\"\"\n",
    "    å°†NLTKè¯æ€§æ ‡æ³¨ç»“æœè½¬æ¢ä¸ºWordNetè¯å½¢è¿˜åŸå™¨ï¼ˆLemmatizerï¼‰å…¼å®¹çš„è¯æ€§æ ‡ç­¾ã€‚\n",
    "    \n",
    "    èƒŒæ™¯ï¼š\n",
    "        NLTKçš„`pos_tag`å‡½æ•°è¿”å›çš„è¯æ€§æ ‡ç­¾éµå¾ªPenn Treebankæ ¼å¼ï¼ˆå¦‚å½¢å®¹è¯ä¸º'JJ'ã€åŠ¨è¯ä¸º'VB'ç­‰ï¼‰ï¼Œ\n",
    "        è€ŒWordNetçš„`lemmatize`æ–¹æ³•éœ€è¦ç‰¹å®šçš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚å½¢å®¹è¯ä¸º`wordnet.ADJ`ã€åŠ¨è¯ä¸º`wordnet.VERB`ç­‰ï¼‰ï¼Œ\n",
    "        å› æ­¤éœ€è¦é€šè¿‡é¦–å­—æ¯æ˜ å°„å®ç°æ ¼å¼è½¬æ¢ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        tagged_tokens (list)ï¼šç”±`nltk.pos_tag`è¿”å›çš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, tag)`ï¼Œ\n",
    "                             å…¶ä¸­`word`æ˜¯è¯è¯­ï¼Œ`tag`æ˜¯Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚'JJ'ã€'VB'ï¼‰ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        listï¼šè½¬æ¢åçš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, wordnet_tag)`ï¼Œ\n",
    "              å…¶ä¸­`wordnet_tag`æ˜¯WordNetå…¼å®¹çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚`wordnet.ADJ`ã€`wordnet.VERB`ï¼‰ã€‚\n",
    "              è‹¥æ ‡ç­¾æ— æ³•åŒ¹é…ï¼Œé»˜è®¤æ˜ å°„ä¸ºåè¯ï¼ˆ`wordnet.NOUN`ï¼‰ã€‚\n",
    "    \n",
    "    æ˜ å°„è§„åˆ™ï¼š\n",
    "        - 'j'ï¼ˆå½¢å®¹è¯ï¼Œå¦‚Pennæ ‡ç­¾'JJ'ï¼‰â†’ `wordnet.ADJ`\n",
    "        - 'v'ï¼ˆåŠ¨è¯ï¼Œå¦‚Pennæ ‡ç­¾'VB'ï¼‰â†’ `wordnet.VERB`\n",
    "        - 'n'ï¼ˆåè¯ï¼Œå¦‚Pennæ ‡ç­¾'NN'ï¼‰â†’ `wordnet.NOUN`\n",
    "        - 'r'ï¼ˆå‰¯è¯ï¼Œå¦‚Pennæ ‡ç­¾'RB'ï¼‰â†’ `wordnet.ADV`\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> from nltk import pos_tag, word_tokenize\n",
    "        >>> tagged = pos_tag(word_tokenize(\"running fast\"))  # [('running', 'VBG'), ('fast', 'RB')]\n",
    "        >>> pos_tag_wordnet(tagged)\n",
    "        [('running', wordnet.VERB), ('fast', wordnet.ADV)]\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    å¯¹æ–‡æœ¬è¿›è¡Œè¯å½¢è¿˜åŸï¼ˆLemmatizationï¼‰ï¼Œå°†è¯è¯­è¿˜åŸä¸ºå…¶åŸºæœ¬å½¢å¼ï¼ˆå¦‚åŠ¨è¯ç¬¬ä¸‰äººç§°â†’åŸå½¢ã€åè¯å¤æ•°â†’å•æ•°ï¼‰ã€‚\n",
    "    \n",
    "    å¤„ç†æµç¨‹ï¼š\n",
    "        1. åˆ†è¯ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºç‹¬ç«‹è¯è¯­ï¼ˆä½¿ç”¨NLTKçš„`word_tokenize`ï¼‰ã€‚\n",
    "        2. è¯æ€§æ ‡æ³¨ï¼šä¸ºæ¯ä¸ªè¯è¯­æ·»åŠ Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆä½¿ç”¨NLTKçš„`pos_tag`ï¼‰ã€‚\n",
    "        3. æ ‡ç­¾è½¬æ¢ï¼šå°†è¯æ€§æ ‡ç­¾è½¬æ¢ä¸ºWordNetå…¼å®¹æ ¼å¼ï¼ˆè°ƒç”¨`pos_tag_wordnet`ï¼‰ã€‚\n",
    "        4. è¯å½¢è¿˜åŸï¼šä½¿ç”¨WordNetè¯å½¢è¿˜åŸå™¨ï¼Œæ ¹æ®è½¬æ¢åçš„è¯æ€§æ ‡ç­¾å¯¹æ¯ä¸ªè¯è¯­è¿›è¡Œè¿˜åŸã€‚\n",
    "        5. æ‹¼æ¥ï¼šå°†è¿˜åŸåçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ï¼ˆè‹±æ–‡ï¼‰ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        strï¼šç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬ï¼Œè¯è¯­å‡ä¸ºåŸºæœ¬å½¢å¼ã€‚\n",
    "    \n",
    "    ä¾èµ–ï¼š\n",
    "        - éœ€è¦åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹å’Œ`averaged_perceptron_tagger`è¯æ€§æ ‡æ³¨æ¨¡å‹ï¼ˆå¯é€šè¿‡`nltk.download()`ä¸‹è½½ï¼‰ã€‚\n",
    "        - éœ€è¦åˆå§‹åŒ–WordNetè¯å½¢è¿˜åŸå™¨ï¼ˆå¦‚`wnl = WordNetLemmatizer()`ï¼‰ã€‚\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> wnl = WordNetLemmatizer()  # å‡è®¾å·²åˆå§‹åŒ–\n",
    "        >>> lemmatize_text(\"Cats are running quickly\")\n",
    "        'cat be run quick'\n",
    "    \"\"\"\n",
    "    pass       \n",
    "\n",
    "def remove_special_characters(text, lang='en', remove_digits=False):\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ã€ç¬¦å·ç­‰ï¼‰ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¿ç•™æ•°å­—ã€‚\n",
    "    \n",
    "    å¤„ç†é€»è¾‘ï¼š\n",
    "        1. æ ¹æ®`remove_digits`å‚æ•°æ„å»ºæ­£åˆ™åŒ¹é…æ¨¡å¼:\n",
    "           - å½“`remove_digits=False`ï¼ˆé»˜è®¤ï¼‰:\n",
    "             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "             - lang='zh': ä¿ç•™ä¸­æ–‡(\\u4e00-\\u9fa5)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "           - å½“`remove_digits=True`:\n",
    "             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "             - lang='zh': ä¿ç•™ä¸­æ–‡(\\u4e00-\\u9fa5)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "        2. ä½¿ç”¨`re.sub`å°†åŒ¹é…åˆ°çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå®ç°ç§»é™¤æ•ˆæœã€‚\n",
    "        3. é¢å¤–å¤„ç†ï¼šç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦ï¼Œç¡®ä¿æ–‡æœ¬æ•´æ´ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚\n",
    "        remove_digits (bool)ï¼šæ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™æ•°å­—ï¼‰ã€‚\n",
    "        remove_special_characters (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™ç‰¹æ®Šå­—ç¬¦ï¼‰\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        strï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦åçš„æ–‡æœ¬ï¼ˆä»…ä¿ç•™æŒ‡å®šå…è®¸çš„å­—ç¬¦ï¼‰ã€‚\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='en')\n",
    "        'Hello  123'\n",
    "        \n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='zh')\n",
    "        'ä¸–ç•Œ 123'\n",
    "        \n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='en', remove_digits=True)\n",
    "        'Hello '\n",
    "        \n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='zh', remove_digits=True)\n",
    "        'ä¸–ç•Œ'    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=None, lang='en'):\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ–‡æœ¬ä¸­çš„åœç”¨è¯ï¼ˆå¦‚è‹±æ–‡çš„\"the\"ã€\"is\"ï¼Œä¸­æ–‡çš„\"çš„\"ã€\"äº†\"ç­‰æ— å®é™…è¯­ä¹‰çš„é«˜é¢‘è¯ï¼‰ï¼Œæ”¯æŒä¸­è‹±æ–‡ã€‚\n",
    "    \n",
    "    å¤„ç†æµç¨‹ï¼š\n",
    "        1. åŠ è½½åœç”¨è¯è¡¨ï¼šè‹¥æœªæä¾›è‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼ˆstopwordsï¼‰ï¼Œåˆ™æ ¹æ®è¯­è¨€ï¼ˆlangï¼‰åŠ è½½é»˜è®¤åœç”¨è¯è¡¨ï¼š\n",
    "           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„è‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('english')ï¼‰ã€‚\n",
    "           - ä¸­æ–‡ï¼šä½¿ç”¨NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('chinese')ï¼‰ã€‚\n",
    "        2. åˆ†è¯ï¼šæ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯å·¥å…·ï¼š\n",
    "           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„`word_tokenize`è¿›è¡Œåˆ†è¯ã€‚\n",
    "           - ä¸­æ–‡ï¼šä½¿ç”¨Jiebaçš„`lcut`ï¼ˆç²¾ç¡®æ¨¡å¼ï¼‰è¿›è¡Œåˆ†è¯ã€‚\n",
    "        3. è¿‡æ»¤åœç”¨è¯ï¼šæ ¹æ®`is_lower_case`åˆ¤æ–­æ˜¯å¦éœ€è¦å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯è¡¨ï¼Œä¿ç•™éåœç”¨è¯ã€‚\n",
    "        4. æ‹¼æ¥ï¼šå°†è¿‡æ»¤åçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚\n",
    "        is_lower_case (bool)ï¼šæ–‡æœ¬æ˜¯å¦å·²è½¬ä¸ºå°å†™ï¼Œé»˜è®¤Falseï¼ˆéœ€å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯ï¼‰ã€‚\n",
    "        stopwords (list, optional)ï¼šè‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼Œè‹¥ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è¡¨ã€‚\n",
    "        lang (str)ï¼šè¯­è¨€ç±»å‹ï¼Œ'en'ï¼ˆè‹±æ–‡ï¼‰æˆ–'zh'ï¼ˆä¸­æ–‡ï¼‰ï¼Œé»˜è®¤'en'ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        strï¼šåˆ‡è¯å¹¶ç§»é™¤åœç”¨è¯åç”¨ç©ºæ ¼ç›¸è¿çš„æ–‡æœ¬ã€‚\n",
    "    \n",
    "    ä¾èµ–ï¼š\n",
    "        - è‹±æ–‡ï¼šéœ€åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹ï¼ˆnltk.download('punkt')ï¼‰å’Œè‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚\n",
    "        - ä¸­æ–‡ï¼šéœ€å®‰è£…Jiebaï¼ˆpip install jiebaï¼‰å’ŒåŠ è½½NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> # è‹±æ–‡ç¤ºä¾‹\n",
    "        >>> remove_stopwords(\"The quick brown fox jumps over the lazy dog\", lang='en')\n",
    "        'quick brown fox jumps lazy dog'\n",
    "        >>> # ä¸­æ–‡ç¤ºä¾‹\n",
    "        >>> remove_stopwords(\"è¿™åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†é‚£åªæ‡’ç‹—\", lang='zh')\n",
    "        'åª æ•æ· æ£•è‰² ç‹ç‹¸ è·³è¿‡ åª æ‡’ ç‹—'\n",
    "    \"\"\"\n",
    "    pass    \n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d086723",
   "metadata": {},
   "source": [
    "# å…³é”®è¾“å‡ºä¸»å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1220bb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write(''' \n",
    "def normalize_doc(doc, \n",
    "                     html_stripping=True, \n",
    "                     contraction_expansion=True,\n",
    "                     accented_char_removal=True, \n",
    "                     text_lower_case=True,\n",
    "                     text_lemmatization=True, \n",
    "                     special_char_removal=True,\n",
    "                     stopword_removal=True, \n",
    "                     remove_digits=False, # Default: keep digits\n",
    "                     zh_simplification=True,\n",
    "                     isDebug=False):\n",
    "    \"\"\"\n",
    "    è§„èŒƒåŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œæ”¯æŒå¤šç§é¢„å¤„ç†æ“ä½œï¼ŒåŒ…æ‹¬HTMLæ ‡ç­¾ç§»é™¤ã€ç¼©å†™æ‰©å±•ã€é‡éŸ³å­—ç¬¦ç§»é™¤ã€\n",
    "    å°å†™è½¬æ¢ã€è¯å½¢è¿˜åŸã€ç‰¹æ®Šå­—ç¬¦ç§»é™¤ã€åœç”¨è¯ç§»é™¤ç­‰ï¼Œé€‚ç”¨äºä¸­è‹±æ–‡æ–‡æœ¬ã€‚\n",
    "    å‚æ•°ï¼š\n",
    "        doc (str): è¾“å…¥æ–‡æ¡£ã€‚\n",
    "        html_stripping (bool): æ˜¯å¦ç§»é™¤HTMLæ ‡ç­¾ï¼Œé»˜è®¤Trueã€‚\n",
    "        contraction_expansion (bool): æ˜¯å¦æ‰©å±•ç¼©å†™ï¼Œé»˜è®¤Trueã€‚\n",
    "        accented_char_removal (bool): æ˜¯å¦ç§»é™¤é‡éŸ³å­—ç¬¦ï¼Œé»˜è®¤Trueã€‚\n",
    "        text_lower_case (bool): æ˜¯å¦å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œé»˜è®¤Trueã€‚\n",
    "        text_lemmatization (bool): æ˜¯å¦è¿›è¡Œè¯å½¢è¿˜åŸï¼Œé»˜è®¤Trueã€‚\n",
    "        special_char_removal (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Trueã€‚\n",
    "        stopword_removal (bool): æ˜¯å¦ç§»é™¤åœç”¨è¯ï¼Œé»˜è®¤Trueã€‚\n",
    "        remove_digits (bool): æ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseã€‚\n",
    "        zh_simplification (bool): æ˜¯å¦å°†ä¸­æ–‡ç¹ä½“è½¬æ¢ä¸ºç®€ä½“ï¼Œé»˜è®¤Trueã€‚\n",
    "        isDebug (bool): æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œé»˜è®¤Falseã€‚\n",
    "    è¿”å›ï¼š\n",
    "        å…ƒç»„ `(doc, lang)`ï¼Œå…¶ä¸­ï¼š\n",
    "            - doc (str)ï¼šé¢„å¤„ç†åçš„æ–‡æœ¬\n",
    "            - lang (str)ï¼šæ–‡æœ¬è¯­è¨€ï¼ˆ'en'/'zh'/'unknown'ï¼‰    \n",
    "    \"\"\"                              \n",
    "    pass\n",
    "''') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94602cb",
   "metadata": {},
   "source": [
    "# å®ç°å…³é”®å­å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9b49c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ–‡æœ¬ä¸­çš„HTMLæ ‡ç­¾å¹¶æå–çº¯æ–‡æœ¬å†…å®¹ï¼ŒåŒæ—¶æ ‡å‡†åŒ–æ¢è¡Œç¬¦ã€‚\n",
    "\n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "        1. ä½¿ç”¨BeautifulSoupè§£æHTMLæ–‡æœ¬ï¼Œç§»é™¤`<iframe>`å’Œ`<script>`æ ‡ç­¾ï¼ˆé€šå¸¸åŒ…å«éæ–‡æœ¬å†…å®¹ï¼‰\n",
    "        2. æå–HTMLä¸­çš„çº¯æ–‡æœ¬å†…å®¹\n",
    "        3. å°†å„ç§æ¢è¡Œç¬¦ï¼ˆ\\rã€\\nã€\\r\\nï¼‰ç»Ÿä¸€æ›¿æ¢ä¸ºå•ä¸ª\\nï¼Œç¡®ä¿æ¢è¡Œæ ¼å¼ä¸€è‡´\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šåŒ…å«HTMLæ ‡ç­¾çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        strï¼šç§»é™¤æ‰€æœ‰HTMLæ ‡ç­¾åçš„çº¯æ–‡æœ¬ï¼Œæ¢è¡Œç¬¦å·²æ ‡å‡†åŒ–ä¸º\\n\n",
    "\n",
    "    ä¾èµ–ï¼š\n",
    "        éœ€è¦å®‰è£…BeautifulSoupåº“ï¼ˆpip install beautifulsoup4ï¼‰\n",
    "\n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> raw_html = \"<p>Hello<br>World!</p><script>alert('test')</script>\"\n",
    "        >>> strip_html_tags(raw_html)\n",
    "        'Hello\\\\nWorld!'\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    \n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"\n",
    "     (NFKD) will apply the compatibility decomposition, i.e.\n",
    "     replace all compatibility characters with their equivalents.\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    \"\"\"\n",
    "    å°†NLTKè¯æ€§æ ‡æ³¨ç»“æœè½¬æ¢ä¸ºWordNetè¯å½¢è¿˜åŸå™¨ï¼ˆLemmatizerï¼‰å…¼å®¹çš„è¯æ€§æ ‡ç­¾ã€‚\n",
    "    \n",
    "    èƒŒæ™¯ï¼š\n",
    "        NLTKçš„`pos_tag`å‡½æ•°è¿”å›çš„è¯æ€§æ ‡ç­¾éµå¾ªPenn Treebankæ ¼å¼ï¼ˆå¦‚å½¢å®¹è¯ä¸º'JJ'ã€åŠ¨è¯ä¸º'VB'ç­‰ï¼‰ï¼Œ\n",
    "        è€ŒWordNetçš„`lemmatize`æ–¹æ³•éœ€è¦ç‰¹å®šçš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚å½¢å®¹è¯ä¸º`wordnet.ADJ`ã€åŠ¨è¯ä¸º`wordnet.VERB`ç­‰ï¼‰ï¼Œ\n",
    "        å› æ­¤éœ€è¦é€šè¿‡é¦–å­—æ¯æ˜ å°„å®ç°æ ¼å¼è½¬æ¢ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        tagged_tokens (list)ï¼šç”±`nltk.pos_tag`è¿”å›çš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, tag)`ï¼Œ\n",
    "                             å…¶ä¸­`word`æ˜¯è¯è¯­ï¼Œ`tag`æ˜¯Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚'JJ'ã€'VB'ï¼‰ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        listï¼šè½¬æ¢åçš„è¯æ€§æ ‡æ³¨åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå…ƒç»„`(word, wordnet_tag)`ï¼Œ\n",
    "              å…¶ä¸­`wordnet_tag`æ˜¯WordNetå…¼å®¹çš„è¯æ€§æ ‡ç­¾ï¼ˆå¦‚`wordnet.ADJ`ã€`wordnet.VERB`ï¼‰ã€‚\n",
    "              è‹¥æ ‡ç­¾æ— æ³•åŒ¹é…ï¼Œé»˜è®¤æ˜ å°„ä¸ºåè¯ï¼ˆ`wordnet.NOUN`ï¼‰ã€‚\n",
    "    \n",
    "    æ˜ å°„è§„åˆ™ï¼š\n",
    "        - 'j'ï¼ˆå½¢å®¹è¯ï¼Œå¦‚Pennæ ‡ç­¾'JJ'ï¼‰â†’ `wordnet.ADJ`\n",
    "        - 'v'ï¼ˆåŠ¨è¯ï¼Œå¦‚Pennæ ‡ç­¾'VB'ï¼‰â†’ `wordnet.VERB`\n",
    "        - 'n'ï¼ˆåè¯ï¼Œå¦‚Pennæ ‡ç­¾'NN'ï¼‰â†’ `wordnet.NOUN`\n",
    "        - 'r'ï¼ˆå‰¯è¯ï¼Œå¦‚Pennæ ‡ç­¾'RB'ï¼‰â†’ `wordnet.ADV`\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> from nltk import pos_tag, word_tokenize\n",
    "        >>> tagged = pos_tag(word_tokenize(\"running fast\"))  # [('running', 'VBG'), ('fast', 'RB')]\n",
    "        >>> pos_tag_wordnet(tagged)\n",
    "        [('running', wordnet.VERB), ('fast', wordnet.ADV)]\n",
    "    \"\"\"\n",
    "    # å®šä¹‰Penn Treebankæ ‡ç­¾é¦–å­—æ¯åˆ°WordNetæ ‡ç­¾çš„æ˜ å°„\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    # è½¬æ¢æ ‡ç­¾ï¼šå–åŸæ ‡ç­¾é¦–å­—æ¯å°å†™ï¼ŒæŸ¥æ˜ å°„è¡¨ï¼›æœªåŒ¹é…åˆ™é»˜è®¤åè¯\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    å¯¹æ–‡æœ¬è¿›è¡Œè¯å½¢è¿˜åŸï¼ˆLemmatizationï¼‰ï¼Œå°†è¯è¯­è¿˜åŸä¸ºå…¶åŸºæœ¬å½¢å¼ï¼ˆå¦‚åŠ¨è¯ç¬¬ä¸‰äººç§°â†’åŸå½¢ã€åè¯å¤æ•°â†’å•æ•°ï¼‰ã€‚\n",
    "    \n",
    "    å¤„ç†æµç¨‹ï¼š\n",
    "        1. åˆ†è¯ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºç‹¬ç«‹è¯è¯­ï¼ˆä½¿ç”¨NLTKçš„`word_tokenize`ï¼‰ã€‚\n",
    "        2. è¯æ€§æ ‡æ³¨ï¼šä¸ºæ¯ä¸ªè¯è¯­æ·»åŠ Penn Treebankæ ¼å¼çš„è¯æ€§æ ‡ç­¾ï¼ˆä½¿ç”¨NLTKçš„`pos_tag`ï¼‰ã€‚\n",
    "        3. æ ‡ç­¾è½¬æ¢ï¼šå°†è¯æ€§æ ‡ç­¾è½¬æ¢ä¸ºWordNetå…¼å®¹æ ¼å¼ï¼ˆè°ƒç”¨`pos_tag_wordnet`ï¼‰ã€‚\n",
    "        4. è¯å½¢è¿˜åŸï¼šä½¿ç”¨WordNetè¯å½¢è¿˜åŸå™¨ï¼Œæ ¹æ®è½¬æ¢åçš„è¯æ€§æ ‡ç­¾å¯¹æ¯ä¸ªè¯è¯­è¿›è¡Œè¿˜åŸã€‚\n",
    "        5. æ‹¼æ¥ï¼šå°†è¿˜åŸåçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ï¼ˆè‹±æ–‡ï¼‰ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        strï¼šç»è¿‡è¯å½¢è¿˜åŸåçš„æ–‡æœ¬ï¼Œè¯è¯­å‡ä¸ºåŸºæœ¬å½¢å¼ã€‚\n",
    "    \n",
    "    ä¾èµ–ï¼š\n",
    "        - éœ€è¦åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹å’Œ`averaged_perceptron_tagger`è¯æ€§æ ‡æ³¨æ¨¡å‹ï¼ˆå¯é€šè¿‡`nltk.download()`ä¸‹è½½ï¼‰ã€‚\n",
    "        - éœ€è¦åˆå§‹åŒ–WordNetè¯å½¢è¿˜åŸå™¨ï¼ˆå¦‚`wnl = WordNetLemmatizer()`ï¼‰ã€‚\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> wnl = WordNetLemmatizer()  # å‡è®¾å·²åˆå§‹åŒ–\n",
    "        >>> lemmatize_text(\"Cats are running quickly\")\n",
    "        'cat be run quick'\n",
    "    \"\"\"\n",
    "    # 1. åˆ†è¯ï¼šå°†æ–‡æœ¬æ‹†åˆ†ä¸ºè¯è¯­åˆ—è¡¨\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # 2. è¯æ€§æ ‡æ³¨ï¼šä¸ºæ¯ä¸ªè¯è¯­æ·»åŠ Penn Treebankæ ¼å¼æ ‡ç­¾\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # 3. è½¬æ¢æ ‡ç­¾ï¼šé€‚é…WordNetè¯å½¢è¿˜åŸå™¨\n",
    "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "    # 4. è¯å½¢è¿˜åŸï¼šæ ¹æ®è¯æ€§æ ‡ç­¾è¿˜åŸæ¯ä¸ªè¯è¯­ï¼Œå†æ‹¼æ¥ä¸ºæ–‡æœ¬\n",
    "    text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text, lang='en', remove_digits=False):\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ã€ç¬¦å·ç­‰ï¼‰ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¿ç•™æ•°å­—ã€‚\n",
    "    \n",
    "    å¤„ç†é€»è¾‘ï¼š\n",
    "        1. æ ¹æ®`remove_digits`å‚æ•°æ„å»ºæ­£åˆ™åŒ¹é…æ¨¡å¼:\n",
    "           - å½“`remove_digits=False`ï¼ˆé»˜è®¤ï¼‰:\n",
    "             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "             - lang='zh': ä¿ç•™ä¸­æ–‡(\\\\u4e00-\\\\u9fa5)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "           - å½“`remove_digits=True`:\n",
    "             - lang='en': ä¿ç•™å­—æ¯(a-zA-Z)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "             - lang='zh': ä¿ç•™ä¸­æ–‡(\\\\u4e00-\\\\u9fa5)ã€æ•°å­—(0-9)å’Œç©ºæ ¼(\\\\s)ï¼Œç§»é™¤å…¶ä»–ä¸ç›¸å…³å­—ç¬¦ã€‚\n",
    "        2. ä½¿ç”¨`re.sub`å°†åŒ¹é…åˆ°çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå®ç°ç§»é™¤æ•ˆæœã€‚\n",
    "        3. é¢å¤–å¤„ç†ï¼šç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦å’Œç©ºç™½å­—ç¬¦ï¼Œç¡®ä¿æ–‡æœ¬æ•´æ´ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚\n",
    "        remove_digits (bool)ï¼šæ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™æ•°å­—ï¼‰ã€‚\n",
    "        remove_special_characters (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Falseï¼ˆä¿ç•™ç‰¹æ®Šå­—ç¬¦ï¼‰\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        strï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦åçš„æ–‡æœ¬ï¼ˆä»…ä¿ç•™æŒ‡å®šå…è®¸çš„å­—ç¬¦ï¼‰ã€‚\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='en')\n",
    "        'Hello  123'\n",
    "        \n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='zh')\n",
    "        'ä¸–ç•Œ 123'\n",
    "        \n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='en', remove_digits=True)\n",
    "        'Hello '\n",
    "        \n",
    "        >>> remove_special_characters(\"Hello, ä¸–ç•Œ! 123 ğŸ™‚ğŸ™‚ğŸ™‚\", lang='zh', remove_digits=True)\n",
    "        'ä¸–ç•Œ'    \n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # æ„å»ºæ­£åˆ™æ¨¡å¼\n",
    "    if lang == 'en':\n",
    "        if remove_digits:\n",
    "            # è‹±æ–‡æ¨¡å¼ï¼šä¿ç•™å­—æ¯å’Œç©ºæ ¼ï¼Œç§»é™¤æ•°å­—\n",
    "            pattern = r'[^a-zA-Z\\s]'\n",
    "        else:\n",
    "            # è‹±æ–‡æ¨¡å¼ï¼šä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼\n",
    "            pattern = r'[^a-zA-Z0-9\\s]'\n",
    "    elif lang == 'zh':\n",
    "        if remove_digits:\n",
    "            # ä¸­æ–‡æ¨¡å¼ï¼šä¿ç•™ä¸­æ–‡å­—ç¬¦å’Œç©ºæ ¼ï¼Œç§»é™¤æ•°å­—\n",
    "            pattern = r'[^\\u4e00-\\u9fa5\\s]'\n",
    "        else:\n",
    "            # ä¸­æ–‡æ¨¡å¼ï¼šä¿ç•™ä¸­æ–‡ã€æ•°å­—å’Œç©ºæ ¼\n",
    "            pattern = r'[^\\u4e00-\\u9fa50-9\\s]'\n",
    "    else:\n",
    "        raise ValueError(\"ä¸æ”¯æŒçš„è¯­è¨€ç±»å‹ï¼Œä»…æ”¯æŒ 'en' æˆ– 'zh'\")\n",
    "    \n",
    "    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    # åŒ¹é…å…¶ä¸­åŒ…å«çš„ä»»æ„ä¸€ä¸ªå­—ç¬¦ã€‚å…·ä½“åŒ…å«çš„å­—ç¬¦æ˜¯ï¼š{ã€.ã€(ã€-ã€)ã€!ã€}    \n",
    "    special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "    \n",
    "    # å°†åŒ¹é…åˆ°çš„å­—ç¬¦æ›¿æ¢ä¸º ç©ºæ ¼ + æ•è·åˆ°çš„å­—ç¬¦ + ç©ºæ ¼\n",
    "    # insert spaces between special characters to isolate them\n",
    "    text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "\n",
    "    # remove extra newlines\n",
    "    text = re.sub(r'[\\r|\\n|\\r\\n]+', ' ', text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=None, lang='en')->str:\n",
    "    \"\"\"\n",
    "    ç§»é™¤æ–‡æœ¬ä¸­çš„åœç”¨è¯ï¼ˆå¦‚è‹±æ–‡çš„\"the\"ã€\"is\"ï¼Œä¸­æ–‡çš„\"çš„\"ã€\"äº†\"ç­‰æ— å®é™…è¯­ä¹‰çš„é«˜é¢‘è¯ï¼‰ï¼Œæ”¯æŒä¸­è‹±æ–‡ã€‚\n",
    "    \n",
    "    å¤„ç†æµç¨‹ï¼š\n",
    "        1. åŠ è½½åœç”¨è¯è¡¨ï¼šè‹¥æœªæä¾›è‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼ˆstopwordsï¼‰ï¼Œåˆ™æ ¹æ®è¯­è¨€ï¼ˆlangï¼‰åŠ è½½é»˜è®¤åœç”¨è¯è¡¨ï¼š\n",
    "           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„è‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('english')ï¼‰ã€‚\n",
    "           - ä¸­æ–‡ï¼šä½¿ç”¨NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.corpus.stopwords.words('chinese')ï¼‰ã€‚\n",
    "        2. åˆ†è¯ï¼šæ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯å·¥å…·ï¼š\n",
    "           - è‹±æ–‡ï¼šä½¿ç”¨NLTKçš„`word_tokenize`è¿›è¡Œåˆ†è¯ã€‚\n",
    "           - ä¸­æ–‡ï¼šä½¿ç”¨Jiebaçš„`lcut`ï¼ˆç²¾ç¡®æ¨¡å¼ï¼‰è¿›è¡Œåˆ†è¯ã€‚\n",
    "        3. è¿‡æ»¤åœç”¨è¯ï¼šæ ¹æ®`is_lower_case`åˆ¤æ–­æ˜¯å¦éœ€è¦å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯è¡¨ï¼Œä¿ç•™éåœç”¨è¯ã€‚\n",
    "        4. æ‹¼æ¥ï¼šå°†è¿‡æ»¤åçš„è¯è¯­é‡æ–°æ‹¼æ¥ä¸ºæ–‡æœ¬ã€‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text (str)ï¼šå¾…å¤„ç†çš„åŸå§‹æ–‡æœ¬ã€‚\n",
    "        is_lower_case (bool)ï¼šæ–‡æœ¬æ˜¯å¦å·²è½¬ä¸ºå°å†™ï¼Œé»˜è®¤Falseï¼ˆéœ€å°†è¯è¯­å°å†™åå†åŒ¹é…åœç”¨è¯ï¼‰ã€‚\n",
    "        stopwords (list, optional)ï¼šè‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼Œè‹¥ä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è¡¨ã€‚\n",
    "        lang (str)ï¼šè¯­è¨€ç±»å‹ï¼Œ'en'ï¼ˆè‹±æ–‡ï¼‰æˆ–'zh'ï¼ˆä¸­æ–‡ï¼‰ï¼Œé»˜è®¤'en'ã€‚\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        strï¼šåˆ‡è¯å¹¶ç§»é™¤åœç”¨è¯åç”¨ç©ºæ ¼ç›¸è¿çš„æ–‡æœ¬ã€‚\n",
    "    \n",
    "    ä¾èµ–ï¼š\n",
    "        - è‹±æ–‡ï¼šéœ€åŠ è½½NLTKçš„`punkt`åˆ†è¯æ¨¡å‹ï¼ˆnltk.download('punkt')ï¼‰å’Œè‹±æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚\n",
    "        - ä¸­æ–‡ï¼šéœ€å®‰è£…Jiebaï¼ˆpip install jiebaï¼‰å’ŒåŠ è½½NLTKçš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆnltk.download('stopwords')ï¼‰ã€‚\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> # è‹±æ–‡ç¤ºä¾‹\n",
    "        >>> remove_stopwords(\"The quick brown fox jumps over the lazy dog\", lang='en')\n",
    "        'quick brown fox jumps lazy dog'\n",
    "        >>> # ä¸­æ–‡ç¤ºä¾‹\n",
    "        >>> remove_stopwords(\"è¿™åªæ•æ·çš„æ£•è‰²ç‹ç‹¸è·³è¿‡äº†é‚£åªæ‡’ç‹—\", lang='zh')\n",
    "        'åª æ•æ· æ£•è‰² ç‹ç‹¸ è·³è¿‡ åª æ‡’ ç‹—'\n",
    "    \"\"\"\n",
    "    # è‹¥æœªæä¾›è‡ªå®šä¹‰åœç”¨è¯è¡¨ï¼Œåˆ™åŠ è½½å¯¹åº”è¯­è¨€çš„é»˜è®¤åœç”¨è¯è¡¨\n",
    "    if not stopwords:\n",
    "        if lang == 'en':\n",
    "            stopwords = nltk.corpus.stopwords.words('english')\n",
    "        elif lang == 'zh':\n",
    "            stopwords = nltk.corpus.stopwords.words('chinese')\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported language. Use 'en' for English or 'zh' for Chinese.\")   \n",
    "\n",
    "    if lang == 'en':\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "    elif lang == 'zh':\n",
    "        tokens = jieba.lcut(text)  # ç²¾ç¡®æ¨¡å¼ï¼Œè¿”å›åˆ—è¡¨\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language. Use 'en' for English or 'zh' for Chinese.\")\n",
    "\n",
    "    # è¿‡æ»¤åœç”¨è¯ï¼šæ ¹æ®æ–‡æœ¬æ˜¯å¦å°å†™ï¼Œå†³å®šæ˜¯å¦å°†è¯è¯­è½¬ä¸ºå°å†™ååŒ¹é…\n",
    "    if lang == 'en':\n",
    "        if is_lower_case:\n",
    "            # æ–‡æœ¬å·²å°å†™ï¼Œç›´æ¥åŒ¹é…åœç”¨è¯è¡¨\n",
    "            filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "        else:\n",
    "            # æ–‡æœ¬æœªå°å†™ï¼Œå°†è¯è¯­è½¬ä¸ºå°å†™åå†åŒ¹é…ï¼ˆé¿å…å› å¤§å°å†™å¯¼è‡´æ¼åˆ¤ï¼‰\n",
    "            filtered_tokens = [token for token in tokens if token.lower() not in stopwords]   \n",
    "    elif lang == 'zh':\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]    \n",
    "                \n",
    "    # å°†è¿‡æ»¤åçš„è¯è¯­æ‹¼æ¥ä¸ºæ–‡æœ¬\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d229d08",
   "metadata": {},
   "source": [
    "# æµ‹è¯•å…³é”®å­å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62126d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorld!\n",
      "cafe cliche naive\n",
      "aeiou AEIOU n c\n",
      "[('running', 'v'), ('fast', 'r')]\n",
      "Cats be run quickly\n",
      "Hello World 123\n",
      "Hello World \n"
     ]
    }
   ],
   "source": [
    "raw_html = \"<p>Hello<br>World!</p><script>alert('test')</script>\"\n",
    "print(strip_html_tags(raw_html))\n",
    "\n",
    "print(remove_accented_chars(\"cafÃ© clichÃ© naÃ¯ve\"))\n",
    "print(remove_accented_chars(\"Ã Ã¨Ã¬Ã²Ã¹ ÃÃ‰ÃÃ“Ãš Ã± Ã§\"))\n",
    "\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(\"running fast\"))  # [('running', 'VBG'), ('fast', 'RB')]\n",
    "print(pos_tag_wordnet(tagged))\n",
    "print(lemmatize_text(\"Cats are running quickly\"))\n",
    "        \n",
    "print(remove_special_characters(\"Hello, World! 123\", remove_digits=False))\n",
    "print(remove_special_characters(\"Hello, World! 123\", remove_digits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257b534",
   "metadata": {},
   "source": [
    "# å®ç°å…³é”®å‡½æ•°ï¼Œè°ƒç”¨å…³é”®å­å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dde177c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc, \n",
    "                     html_stripping=True, \n",
    "                     contraction_expansion=True,\n",
    "                     accented_char_removal=True, \n",
    "                     text_lower_case=True,\n",
    "                     text_lemmatization=True, \n",
    "                     special_char_removal=True,\n",
    "                     stopword_removal=True, \n",
    "                     remove_digits=False, # Default: keep digits\n",
    "                     zh_simplification=True,\n",
    "                     isDebug=False):\n",
    "    \"\"\"\n",
    "    è§„èŒƒåŒ–æ–‡æœ¬è¯­æ–™åº“ï¼Œæ”¯æŒå¤šç§é¢„å¤„ç†æ“ä½œï¼ŒåŒ…æ‹¬HTMLæ ‡ç­¾ç§»é™¤ã€ç¼©å†™æ‰©å±•ã€é‡éŸ³å­—ç¬¦ç§»é™¤ã€\n",
    "    å°å†™è½¬æ¢ã€è¯å½¢è¿˜åŸã€ç‰¹æ®Šå­—ç¬¦ç§»é™¤ã€åœç”¨è¯ç§»é™¤ç­‰ï¼Œé€‚ç”¨äºä¸­è‹±æ–‡æ–‡æœ¬ã€‚\n",
    "    å‚æ•°ï¼š\n",
    "        doc (str): è¾“å…¥æ–‡æ¡£ã€‚\n",
    "        html_stripping (bool): æ˜¯å¦ç§»é™¤HTMLæ ‡ç­¾ï¼Œé»˜è®¤Trueã€‚\n",
    "        contraction_expansion (bool): æ˜¯å¦æ‰©å±•ç¼©å†™ï¼Œé»˜è®¤Trueã€‚\n",
    "        accented_char_removal (bool): æ˜¯å¦ç§»é™¤é‡éŸ³å­—ç¬¦ï¼Œé»˜è®¤Trueã€‚\n",
    "        text_lower_case (bool): æ˜¯å¦å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œé»˜è®¤Trueã€‚\n",
    "        text_lemmatization (bool): æ˜¯å¦è¿›è¡Œè¯å½¢è¿˜åŸï¼Œé»˜è®¤Trueã€‚\n",
    "        special_char_removal (bool): æ˜¯å¦ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé»˜è®¤Trueã€‚\n",
    "        stopword_removal (bool): æ˜¯å¦ç§»é™¤åœç”¨è¯ï¼Œé»˜è®¤Trueã€‚\n",
    "        remove_digits (bool): æ˜¯å¦ç§»é™¤æ•°å­—ï¼Œé»˜è®¤Falseã€‚\n",
    "        zh_simplification (bool): æ˜¯å¦å°†ä¸­æ–‡ç¹ä½“è½¬æ¢ä¸ºç®€ä½“ï¼Œé»˜è®¤Trueã€‚\n",
    "        isDebug (bool): æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œé»˜è®¤Falseã€‚\n",
    "    è¿”å›ï¼š\n",
    "        å…ƒç»„ `(doc, lang)`ï¼Œå…¶ä¸­ï¼š\n",
    "            - doc (str)ï¼šé¢„å¤„ç†åçš„æ–‡æœ¬\n",
    "            - lang (str)ï¼šæ–‡æœ¬è¯­è¨€ï¼ˆ'en'/'zh'/'unknown'ï¼‰    \n",
    "    \"\"\"          \n",
    "    # normalize the document\n",
    "    if not doc:  # è·³è¿‡ç©ºæ–‡æ¡£\n",
    "        print(\"è­¦å‘Šï¼šè¾“å…¥æ–‡æ¡£ä¸ºç©ºï¼Œè·³è¿‡é¢„å¤„ç†ã€‚\")\n",
    "        return doc, 'unknown'\n",
    "        \n",
    "    # 1. æ£€æµ‹è¯­è¨€\n",
    "    lang = detect_language(doc)\n",
    "\n",
    "    if isDebug:\n",
    "        print(f\"æ£€æµ‹åˆ°çš„è¯­è¨€: {lang}\")\n",
    "    \n",
    "    if lang not in ['en', 'zh']:\n",
    "        print(f\"è­¦å‘Šï¼šæ— æ³•è¯†åˆ«æ–‡æ¡£è¯­è¨€ï¼Œè·³è¿‡é¢„å¤„ç†ï¼š{doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "    \n",
    "    # strip HTML\n",
    "    if html_stripping:\n",
    "        doc = strip_html_tags(doc)\n",
    "        if isDebug:\n",
    "            print(f\"HTMLæ ‡ç­¾ç§»é™¤å: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "        \n",
    "    if lang == 'en':\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "            if isDebug:\n",
    "                print(f\"é‡éŸ³å­—ç¬¦ç§»é™¤å: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "    elif lang == 'zh':\n",
    "        # Chinese text specific processing\n",
    "        # ç¹ä½“è½¬ç®€ä½“\n",
    "        if zh_simplification:\n",
    "            doc = cc_zh.convert(doc)\n",
    "            if isDebug: \n",
    "                print(f\"ä¸­æ–‡ç¹ä½“è½¬æ¢ä¸ºç®€ä½“: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "    else:\n",
    "        # æœªçŸ¥è¯­è¨€ï¼Œè·³è¿‡åç»­å¤„ç†\n",
    "        print(\"è­¦å‘Šï¼šæ— æ³•è¯†åˆ«æ–‡æ¡£è¯­è¨€ï¼Œè·³è¿‡é¢„å¤„ç†\")\n",
    "        return doc, 'unknown'\n",
    "    \n",
    "    # expand contractions\n",
    "    if contraction_expansion:\n",
    "        doc = contractions.fix(doc)\n",
    "\n",
    "        if isDebug:\n",
    "            print(f\"ç¼©å†™æ‰©å±•å: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "        \n",
    "    # lowercase the text\n",
    "    if text_lower_case:\n",
    "        doc = doc.lower()\n",
    "\n",
    "        if isDebug:\n",
    "            print(f\"å°å†™è½¬æ¢å: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "        \n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "            if isDebug:\n",
    "                print(f\"è¯å½¢è¿˜åŸå: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "\n",
    "    # remove special characters and\\or digits\n",
    "    if special_char_removal:\n",
    "        doc = remove_special_characters(doc, lang, remove_digits=remove_digits)\n",
    "        \n",
    "        if isDebug:\n",
    "            print(f\"ç‰¹æ®Šå­—ç¬¦ç§»é™¤å: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "\n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        doc = remove_stopwords(doc, is_lower_case=text_lower_case, lang=lang)\n",
    "        \n",
    "        if isDebug:\n",
    "            print(f\"åœç”¨è¯ç§»é™¤å: {doc[:50]}...\\n{doc[len(doc)-50:]}\")\n",
    "                \n",
    "    return doc, lang \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a7b1b",
   "metadata": {},
   "source": [
    "# è·å–æ–‡æœ¬å†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bb3d6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write('''\n",
    "def fetch_gutenberg_text(file_path=None):\n",
    "    \"\"\"\n",
    "    è¯»å–æœ¬åœ°å¤è…¾å ¡txtæ–‡ä»¶ï¼Œæå–æ­£æ–‡å†…å®¹ï¼ˆå»é™¤é¦–å°¾å…ƒæ•°æ®æ ‡è®°ï¼‰\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        file_path (str): æœ¬åœ°txtæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ './data/1342-0.txt'\n",
    "    \n",
    "    è¿”å›ï¼š\n",
    "        str: æå–çš„æ­£æ–‡å†…å®¹ï¼›è‹¥æ–‡ä»¶è¯»å–å¤±è´¥/æ— æœ‰æ•ˆæ ‡è®°ï¼Œè¿”å›None\n",
    "    \n",
    "    ç¤ºä¾‹ï¼š\n",
    "        >>> content = fetch_gutenberg_text('./data/1342-0.txt')\n",
    "        >>> print(content[:500])  # æ‰“å°æ­£æ–‡å‰500å­—ç¬¦\n",
    "           \n",
    "    è¯´æ˜: \n",
    "        ä½ å¯ä»¥è‡ªå·±å®ç°è¾“å…¥ fetch_gutenberg_text(link='https://www.gutenberg.org/files/1342/1342-0.txt'):\n",
    "        >>> content = fetch_gutenberg_text('https://www.gutenberg.org/files/1342/1342-0.txt')\n",
    "        >>> print(content[:500])  # æ‰“å°æ­£æ–‡å‰500å­—ç¬¦\n",
    "    \"\"\"\n",
    "    # 1. éªŒè¯æ–‡ä»¶æ ¼å¼ï¼ˆä»…æ”¯æŒtxtï¼‰\n",
    "    if not file_path.endswith('.txt'):\n",
    "        print(f\"é”™è¯¯ï¼š{file_path} ä¸æ˜¯.txtæ ¼å¼æ–‡ä»¶ï¼Œä»…æ”¯æŒæ–‡æœ¬æ–‡ä»¶\")\n",
    "        return None\n",
    "\n",
    "    # 2. æ­£åˆ™åŒ¹é…å¤è…¾å ¡æ ‡å‡†æ ‡è®°ï¼ˆæå–æ­£æ–‡æ ¸å¿ƒï¼‰\n",
    "    # åŒ¹é… \"*** START OF THE PROJECT GUTENBERG EBOOK ... ***\" åŠå˜ä½“\n",
    "    start_pat = re.compile(\n",
    "        r'\\*{3}\\s+START OF (?:THE|THIS)?\\s+PROJECT GUTENBERG EBOOK.*?\\*{3}',\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    # åŒ¹é… \"*** END OF THE PROJECT GUTENBERG EBOOK ... ***\" åŠå˜ä½“\n",
    "    end_pat = re.compile(\n",
    "        r'\\*{3}\\s+END OF (?:THE|THIS)?\\s+PROJECT GUTENBERG EBOOK.*?\\*{3}',\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "\n",
    "    # 3. è¯»å–æœ¬åœ°æ–‡ä»¶ï¼ˆé€‚é…å¤è…¾å ¡å¸¸è§ç¼–ç ï¼šutf-8 / latin-1ï¼‰\n",
    "    try:\n",
    "        print(f\"æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶ï¼š{file_path}\")\n",
    "        \n",
    "        # ä¼˜å…ˆå°è¯•utf-8ç¼–ç ï¼ˆå¤è…¾å ¡ç°ä»£æ–‡æœ¬å¸¸ç”¨ï¼‰\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            print(\"æ–‡ä»¶ç¼–ç ï¼šutf-8ï¼ˆè¯»å–æˆåŠŸï¼‰\")\n",
    "        \n",
    "        # è‹¥utf-8è§£ç å¤±è´¥ï¼Œå°è¯•latin-1ï¼ˆå¤è…¾å ¡æ—©æœŸè‹±æ–‡æ–‡æœ¬å¸¸ç”¨ï¼‰\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                text = f.read()\n",
    "            print(\"æ–‡ä»¶ç¼–ç ï¼šlatin-1ï¼ˆè¯»å–æˆåŠŸï¼‰\")\n",
    "\n",
    "    # æ•è·æœ¬åœ°æ–‡ä»¶è¯»å–çš„å¸¸è§é”™è¯¯\n",
    "    except FileNotFoundError:\n",
    "        print(f\"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®\")\n",
    "        return None\n",
    "    except PermissionError:\n",
    "        print(f\"é”™è¯¯ï¼šæ— æƒé™è¯»å–æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"é”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ - {str(e)[:50]}...\")\n",
    "        return None\n",
    "\n",
    "    # 4. æå–æ­£æ–‡ï¼ˆåŸºäºå¤è…¾å ¡æ ‡è®°ï¼‰\n",
    "    start_match = start_pat.search(text)\n",
    "    end_match = end_pat.search(text)\n",
    "\n",
    "    # å¤„ç†æ ‡è®°ä¸å­˜åœ¨/ä½ç½®å¼‚å¸¸çš„æƒ…å†µ\n",
    "    if not start_match:\n",
    "        print(\"è­¦å‘Šï¼šæœªæ‰¾åˆ°å¤è…¾å ¡æ­£æ–‡å¼€å§‹æ ‡è®°ï¼ˆ*** START OF ... ***ï¼‰\")\n",
    "        # å¯é€‰ï¼šè¿”å›å…¨æ–‡ï¼ˆè‹¥ç”¨æˆ·å¸Œæœ›ä¿ç•™å®Œæ•´æ–‡ä»¶å†…å®¹ï¼‰\n",
    "        # return text.strip()\n",
    "        return None\n",
    "    if not end_match:\n",
    "        print(\"è­¦å‘Šï¼šæœªæ‰¾åˆ°å¤è…¾å ¡æ­£æ–‡ç»“æŸæ ‡è®°ï¼ˆ*** END OF ... ***ï¼‰\")\n",
    "        # å¯é€‰ï¼šè¿”å›å¼€å§‹æ ‡è®°åçš„å†…å®¹\n",
    "        # return text[start_match.end():].strip()\n",
    "        return None\n",
    "    if start_match.end() >= end_match.start():\n",
    "        print(\"é”™è¯¯ï¼šå¼€å§‹æ ‡è®°ä½ç½®åœ¨ç»“æŸæ ‡è®°ä¹‹åï¼Œæ— æ³•æå–æ­£æ–‡\")\n",
    "        return None\n",
    "\n",
    "    # æå–æ ‡è®°é—´çš„æ­£æ–‡ï¼ˆå»é™¤é¦–å°¾ç©ºæ ¼ï¼‰\n",
    "    content = text[start_match.end():end_match.start()].strip()\n",
    "    print(f\"æ­£æ–‡æå–æˆåŠŸï¼æ­£æ–‡é•¿åº¦ï¼š{len(content)} å­—ç¬¦\")\n",
    "    return content\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7ec8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write('''\n",
    "from pathlib import Path  # ç¡®ä¿å·²å¯¼å…¥\n",
    "\n",
    "def save_processed_text(original_path, processed_text):\n",
    "    \"\"\"\n",
    "    å°†é¢„å¤„ç†åçš„æ–‡æœ¬ä¿å­˜è‡³æ–°æ–‡ä»¶ï¼Œå‘½åè§„åˆ™ï¼šåŸæ–‡ä»¶xxx.txt â†’ xxx-p.txt\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        original_path (str): åŸæ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ './data/1342-0.txt'ï¼‰\n",
    "        processed_text (str): é¢„å¤„ç†åçš„æ–‡æœ¬å†…å®¹\n",
    "    \"\"\"\n",
    "    # è½¬æ¢ä¸ºPathå¯¹è±¡ï¼Œæ–¹ä¾¿å¤„ç†è·¯å¾„\n",
    "    orig_path = Path(original_path)\n",
    "    \n",
    "    # ç”Ÿæˆæ–°æ–‡ä»¶åï¼šåŸæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰+ \"-p\" + æ‰©å±•å\n",
    "    new_filename = f\"{orig_path.stem}-p{orig_path.suffix}\"  # å¦‚ \"1342-0-p.txt\"\n",
    "    \n",
    "    # æ„å»ºæ–°æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ï¼ˆä¸åŸæ–‡ä»¶åŒç›®å½•ï¼‰\n",
    "    new_path = orig_path.parent / new_filename  # å¦‚ './data/1342-0-p.txt'\n",
    "    \n",
    "    try:\n",
    "        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨ï¼ˆè‹¥ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰\n",
    "        new_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # å†™å…¥é¢„å¤„ç†åçš„æ–‡æœ¬ï¼ˆä½¿ç”¨utf-8ç¼–ç ï¼Œé¿å…ä¸­æ–‡ä¹±ç ï¼‰\n",
    "        with open(new_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(processed_text)\n",
    "        \n",
    "        print(f\"âœ… é¢„å¤„ç†æ–‡æœ¬å·²ä¿å­˜è‡³ï¼š{new_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ {new_path}ï¼š{str(e)}\")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d08927e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ æœªè¿½åŠ åˆ°æ–‡ä»¶ï¼ˆå¼€å…³å…³é—­ï¼‰\n"
     ]
    }
   ],
   "source": [
    "auto_write('''\n",
    "def test_english_contractions(processed: str)->list:\n",
    "    \"\"\"\n",
    "    æ£€æŸ¥å¤„ç†åçš„æ–‡æœ¬ä¸­æ˜¯å¦ä»åŒ…å«æœªæ‰©å±•çš„è‹±æ–‡ç¼©å†™æ ¼å¼\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        processed: å¤„ç†åçš„æ–‡æœ¬\n",
    "    è¿”å›ï¼š\n",
    "        é”™è¯¯åˆ—è¡¨ï¼ˆåŒ…å«ä»å­˜åœ¨çš„ç¼©å†™æ ¼å¼ï¼‰\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    # ç»Ÿä¸€è½¬ä¸ºå°å†™æ£€æŸ¥ï¼ˆå¿½ç•¥å¤§å°å†™å½±å“ï¼‰\n",
    "    processed_lower = processed.lower()\n",
    "    \n",
    "    # éå†contractionsåŒ…æ”¯æŒçš„æ‰€æœ‰ç¼©å†™\n",
    "    for contraction in contractions.contractions_dict.keys():\n",
    "        # 1. æ£€æŸ¥å¤„ç†åçš„æ–‡æœ¬ä¸­æ˜¯å¦å­˜åœ¨è¯¥ç¼©å†™ï¼ˆå®Œæ•´å•è¯åŒ¹é…ï¼‰\n",
    "        # æ­£åˆ™ï¼š\\b ç¡®ä¿åŒ¹é…ç‹¬ç«‹å•è¯ï¼Œre.escapeå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚æ’‡å·'ï¼‰\n",
    "        contraction_pattern = re.compile(rf'\\b{re.escape(contraction)}\\b')\n",
    "        if contraction_pattern.search(processed_lower):\n",
    "            # è‹¥å­˜åœ¨æœªæ‰©å±•çš„ç¼©å†™ï¼Œè®°å½•é”™è¯¯\n",
    "            errors.append(f\"å¤„ç†åçš„æ–‡æœ¬ä»åŒ…å«æœªæ‰©å±•çš„ç¼©å†™: '{contraction}'\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "def test_stopwords(processed:str, lang='en')->list:\n",
    "    \"\"\"\n",
    "    å‡è®¾processedæ˜¯å·²ç»åˆ‡è¯ç”¨ç©ºæ ¼åˆ†éš”è¿æ¥çš„å­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    if lang == 'en':\n",
    "        stopwords_en = set(nltk.corpus.stopwords.words('english'))\n",
    "    elif lang == 'zh':\n",
    "        stopwords_zh = set(nltk.corpus.stopwords.words('chinese'))\n",
    "    else: \n",
    "        raise ValueError(\"Unsupported language. Use 'en' or 'zh'.\")\n",
    "            \n",
    "    # tokens = nltk.word_tokenize(processed) if lang == 'en' else jieba.lcut(processed)\n",
    "    tokens = processed.split()\n",
    "\n",
    "    if lang == 'en' and any(token in stopwords_en for token in tokens):\n",
    "        errors.append(\"è‹±æ–‡åœç”¨è¯æœªç§»é™¤\")\n",
    "    if lang == 'zh' and any(token in stopwords_zh for token in tokens):\n",
    "        errors.append(\"ä¸­æ–‡åœç”¨è¯æœªç§»é™¤\")\n",
    "        \n",
    "    return errors\n",
    "\n",
    "def test_preprocessed_text(processed, lang)->list:\n",
    "    \"\"\"éªŒè¯é¢„å¤„ç†ç»“æœ\"\"\"\n",
    "    errors = []\n",
    "    # æ£€æŸ¥HTMLæ ‡ç­¾æ˜¯å¦ç§»é™¤ï¼ˆå‡è®¾åŸå§‹æ–‡æœ¬å«HTMLæ ‡ç­¾ï¼Œæ­¤å¤„ç®€åŒ–ä¸ºæ£€æŸ¥ç‰¹æ®Šæ ‡ç­¾å­—ç¬¦ï¼‰\n",
    "    if '<' in processed or '>' in processed:\n",
    "        errors.append(\"HTMLæ ‡ç­¾æœªå®Œå…¨ç§»é™¤\")\n",
    "    \n",
    "    # æ£€æŸ¥å°å†™è½¬æ¢\n",
    "    if any(c.isupper() for c in processed) and lang == 'en':\n",
    "        errors.append(\"è‹±æ–‡æ–‡æœ¬æœªè½¬ä¸ºå°å†™\")\n",
    "    \n",
    "    # æ£€æŸ¥è‹±æ–‡ç¼©å†™æ‰©å±•ï¼ˆç¤ºä¾‹ï¼šdon't â†’ do notï¼‰\n",
    "    if lang == 'en':\n",
    "        err_test_english_contractions = test_english_contractions(processed) \n",
    "        if len(err_test_english_contractions)>0: # not empty\n",
    "            errors.extend(err_test_english_contractions)\n",
    "    \n",
    "    # æ£€æŸ¥åœç”¨è¯ç§»é™¤ï¼ˆè‹±æ–‡ç¤ºä¾‹ï¼š'the'ï¼›ä¸­æ–‡ç¤ºä¾‹ï¼š'çš„'ï¼‰\n",
    "    err_test_stopwords = test_stopwords(processed, lang)\n",
    "    if len(err_test_stopwords)>0:\n",
    "        errors.extend(err_test_stopwords)\n",
    "\n",
    "    # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦ç§»é™¤ï¼ˆç¤ºä¾‹ï¼š'!' '?' ç­‰ï¼‰\n",
    "    if any(c in '!@#$%^&*()_+' for c in processed):\n",
    "        errors.append(\"ç‰¹æ®Šå­—ç¬¦æœªç§»é™¤\")\n",
    "    \n",
    "    return errors\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24518ce",
   "metadata": {},
   "source": [
    "# æµ‹è¯•è·å–æ–‡æœ¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è·å–æµ‹è¯•æ•°æ® ===\n",
      "æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶ï¼š./data/1342-0.txt\n",
      "æ–‡ä»¶ç¼–ç ï¼šutf-8ï¼ˆè¯»å–æˆåŠŸï¼‰\n",
      "æ­£æ–‡æå–æˆåŠŸï¼æ­£æ–‡é•¿åº¦ï¼š728713 å­—ç¬¦\n",
      "âœ… é¢„å¤„ç†æ–‡æœ¬å·²ä¿å­˜è‡³ï¼šdata/1342-0-p.txt\n",
      "\n",
      "=== ä¹¦å: PRIDE and PREJUDICE ===\n",
      "\n",
      "=== å¤„ç†åæ–‡æœ¬ï¼ˆè¯­è¨€ï¼šenï¼‰===\n",
      "illustration george allen publisher 156 char cross road london ruskin house illustration reading jan...\n",
      "æ–‡æœ¬é•¿åº¦ï¼š401496å­—ç¬¦\n",
      "æ–‡æœ¬å‰10é«˜é¢‘è¯ï¼š[('mr', 809), ('elizabeth', 614), ('say', 614), ('could', 531), ('would', 486), ('know', 391), ('darcy', 385), ('mrs', 353), ('bennet', 349), ('make', 343)]\n",
      "æ–‡æœ¬å‰10é•¿è¯ï¼š[('charactercreation', 17), ('communicativeness', 17), ('disinterestedness', 17), ('eighteenthcentury', 17), ('fordycebelectured', 17), ('misrepresentation', 17), ('agreeablelooking', 16), ('discontentedness', 16), ('fellowtravellers', 16), ('greatgrandmother', 16)]\n",
      "âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡\n",
      "æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶ï¼š./data/24264-0.txt\n",
      "æ–‡ä»¶ç¼–ç ï¼šutf-8ï¼ˆè¯»å–æˆåŠŸï¼‰\n",
      "æ­£æ–‡æå–æˆåŠŸï¼æ­£æ–‡é•¿åº¦ï¼š906088 å­—ç¬¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/6_/9scks0f16w9ck73gbx9wznbh0000gn/T/jieba.cache\n",
      "Loading model cost 0.272 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é¢„å¤„ç†æ–‡æœ¬å·²ä¿å­˜è‡³ï¼šdata/24264-0-p.txt\n",
      "\n",
      "=== ä¹¦å: A Dream Of Red Mansions ===\n",
      "\n",
      "=== å¤„ç†åæ–‡æœ¬ï¼ˆè¯­è¨€ï¼šzhï¼‰===\n",
      "ç¬¬ä¸€å›   ç”„å£«éš æ¢¦å¹» è¯†é€šçµ   è´¾é›¨æ‘ é£å°˜ æ€€ é—ºç§€   å¼€å· ç¬¬ä¸€å› ä½œè€… è‡ªäº‘å›  æ›¾ å†è¿‡ ä¸€ç•ª æ¢¦å¹» çœŸäº‹ éšå»   å€Ÿ   é€šçµ   è¯´ æ’°æ­¤ çŸ³å¤´è®° ä¸€ä¹¦ æ•…æ›°   ç”„å£«éš   ä½†ä¹¦...\n",
      "æ–‡æœ¬é•¿åº¦ï¼š947985å­—ç¬¦\n",
      "æ–‡æœ¬å‰10é«˜é¢‘è¯ï¼š[('é“', 6491), ('è¯´', 6111), ('ä¸', 4107), ('å»', 3864), ('å®ç‰', 3618), ('è‘—', 3175), ('ä¾¿', 3173), ('äºº', 3005), ('éƒ½', 2614), ('ç¬‘', 2487)]\n",
      "æ–‡æœ¬å‰10é•¿è¯ï¼š[('ä¸€ä¸ªå·´æŒæ‹ä¸å“', 7), ('ä¸æ˜¯å†¤å®¶ä¸èšå¤´', 7), ('äººæ€•å‡ºåçŒªæ€•å£®', 7), ('åƒé‡Œå§»ç¼˜ä¸€çº¿ç‰µ', 7), ('å¾—é¥¶äººå¤„ä¸”é¥¶äºº', 7), ('å¿ƒæœ‰ä½™è€ŒåŠ›ä¸è¶³', 7), ('æƒ…äººçœ¼é‡Œå‡ºè¥¿æ–½', 7), ('æ•¢æŠŠçš‡å¸æ‹‰ä¸‹é©¬', 7), ('æœºå…³ç®—å°½å¤ªèªæ˜', 7), ('çŸ¥äººçŸ¥é¢ä¸çŸ¥å¿ƒ', 7)]\n",
      "âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡\n",
      "æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶ï¼š./data/23950-0.txt\n",
      "æ–‡ä»¶ç¼–ç ï¼šutf-8ï¼ˆè¯»å–æˆåŠŸï¼‰\n",
      "æ­£æ–‡æå–æˆåŠŸï¼æ­£æ–‡é•¿åº¦ï¼š622115 å­—ç¬¦\n",
      "âœ… é¢„å¤„ç†æ–‡æœ¬å·²ä¿å­˜è‡³ï¼šdata/23950-0-p.txt\n",
      "\n",
      "=== ä¹¦å: Romance of the Three Kingdoms ===\n",
      "\n",
      "=== å¤„ç†åæ–‡æœ¬ï¼ˆè¯­è¨€ï¼šzhï¼‰===\n",
      "  ç¬¬ä¸€å› å®´ æ¡ƒå›­ è±ªæ° ä¸‰ ç»“ä¹‰ æ–© é»„å·¾ è‹±é›„ é¦– ç«‹åŠŸ   è¯æ›°   æ»šæ»š é•¿æ±Ÿ ä¸œ é€æ°´ æµªèŠ± æ·˜å°½ è‹±é›„ æ˜¯éæˆè´¥ è½¬å¤´ ç©º é’å±± ä¾æ—§ å‡ åº¦ å¤•é˜³çº¢ ç™½   å‘ æ¸”æ¨µ æ±Ÿæ¸š ä¸Šæƒ¯ çœ‹ ç§‹...\n",
      "æ–‡æœ¬é•¿åº¦ï¼š703814å­—ç¬¦\n",
      "æ–‡æœ¬å‰10é«˜é¢‘è¯ï¼š[('æ›°', 7700), ('å¾', 1827), ('ä¸', 1630), ('å»', 1226), ('çš†', 1131), ('äºº', 1064), ('è§', 952), ('æ›¹æ“', 911), ('é‚', 828), ('å­”æ˜', 802)]\n",
      "æ–‡æœ¬å‰10é•¿è¯ï¼š[('ä¸‰åˆ†å¤©ä¸‹æœ‰å…¶äºŒ', 7), ('å‹¿ä»¥æ¶å°è€Œä¸ºä¹‹', 7), ('å°ä¸å¿åˆ™ä¹±å¤§è°‹', 7), ('å¼ºä¸­è‡ªæœ‰å¼ºä¸­æ‰‹', 7), ('å¿ è¨€é€†è€³åˆ©äºè¡Œ', 7), ('æŒŸå¤©å­ä»¥ä»¤è¯¸ä¾¯', 7), ('ç½®ä¹‹æ­»åœ°è€Œåç”Ÿ', 7), ('è‰¯è¯è‹¦å£åˆ©äºç—…', 7), ('è‹±é›„æ— ç”¨æ­¦ä¹‹åœ°', 7), ('è¯†æ—¶åŠ¡è€…ä¸ºä¿Šæ°', 7)]\n",
      "âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡\n"
     ]
    }
   ],
   "source": [
    "# 1. è·å–æµ‹è¯•æ•°æ®\n",
    "print(\"=== è·å–æµ‹è¯•æ•°æ® ===\")\n",
    "\n",
    "booknames = ['PRIDE and PREJUDICE', 'A Dream Of Red Mansions', 'Romance of the Three Kingdoms']\n",
    "file_paths = ['./data/1342-0.txt', './data/24264-0.txt', './data/23950-0.txt']\n",
    "\n",
    "docs = []\n",
    "processed_docs = []\n",
    "langs = []\n",
    "\n",
    "for i, file_path in enumerate(file_paths):\n",
    "    ori_doc = fetch_gutenberg_text(file_path)\n",
    "    if ori_doc:\n",
    "        docs.append(ori_doc)\n",
    "\n",
    "    processed_doc, lang = normalize_doc(ori_doc, isDebug=False)\n",
    "    save_processed_text(file_path, processed_doc)\n",
    "    processed_docs.append(processed_doc)\n",
    "    langs.append(lang)\n",
    "\n",
    "    bookname = booknames[i]\n",
    "    n, k = 10, 10  # ç»Ÿè®¡å‰né«˜é¢‘è¯å’Œå‰ké•¿è¯\n",
    "\n",
    "\n",
    "    print(f\"\\n=== ä¹¦å: {bookname} ===\")\n",
    "    print(f\"\\n=== å¤„ç†åæ–‡æœ¬ï¼ˆè¯­è¨€ï¼š{lang}ï¼‰===\\n{processed_doc[:100]}...\\næ–‡æœ¬é•¿åº¦ï¼š{len(processed_doc)}å­—ç¬¦\")\n",
    "    top_n_words, longest_k_words = get_statistics(processed_doc, n, k, lang=lang)\n",
    "    print(f\"æ–‡æœ¬å‰10é«˜é¢‘è¯ï¼š{top_n_words}\")\n",
    "    print(f\"æ–‡æœ¬å‰10é•¿è¯ï¼š{longest_k_words}\")\n",
    "\n",
    "    test_errors = test_preprocessed_text(processed_doc, lang)\n",
    "    if not test_errors:\n",
    "        print(\"âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡\")\n",
    "    else:\n",
    "        print(\"âŒ é”™è¯¯ï¼š\")\n",
    "        for e in test_errors:\n",
    "            print(f\"- {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6dc741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æ£€æµ‹ä»£ç \n",
    "def load_file(file_path:str)->str: \n",
    "    try:\n",
    "        print(f\"æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶ï¼š{file_path}\")\n",
    "        \n",
    "        # ä¼˜å…ˆå°è¯•utf-8ç¼–ç ï¼ˆå¤è…¾å ¡ç°ä»£æ–‡æœ¬å¸¸ç”¨ï¼‰\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            print(\"æ–‡ä»¶ç¼–ç ï¼šutf-8ï¼ˆè¯»å–æˆåŠŸï¼‰\")\n",
    "        \n",
    "        # è‹¥utf-8è§£ç å¤±è´¥ï¼Œå°è¯•latin-1ï¼ˆå¤è…¾å ¡æ—©æœŸè‹±æ–‡æ–‡æœ¬å¸¸ç”¨ï¼‰\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                text = f.read()\n",
    "            print(\"æ–‡ä»¶ç¼–ç ï¼šlatin-1ï¼ˆè¯»å–æˆåŠŸï¼‰\")\n",
    "\n",
    "    # æ•è·æœ¬åœ°æ–‡ä»¶è¯»å–çš„å¸¸è§é”™è¯¯\n",
    "    except FileNotFoundError:\n",
    "        print(f\"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®\")\n",
    "        return None\n",
    "    except PermissionError:\n",
    "        print(f\"é”™è¯¯ï¼šæ— æƒé™è¯»å–æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"é”™è¯¯ï¼šè¯»å–æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ - {str(e)[:50]}...\")\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "# å‡è®¾ä½ å·²ç»æœ‰äº† filtered_tokens åˆ—è¡¨ï¼ˆä¾‹å¦‚ä» remove_stopwords å‡½æ•°ä¸­è·å–ï¼‰\n",
    "# ç¤ºä¾‹ï¼šfiltered_tokens = ['æ•æ·', 'æ£•è‰²', 'ç‹ç‹¸', 'è·³è¿‡', 'æ‡’ç‹—']\n",
    "\n",
    "def save_tokens(filtered_tokens, file_path=\"filtered_tokens.txt\"):\n",
    "    \"\"\"\n",
    "    å°†è¿‡æ»¤åçš„ tokens ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ª token\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        filtered_tokens (list): è¿‡æ»¤åçš„è¯è¯­åˆ—è¡¨\n",
    "        file_path (str): ä¿å­˜çš„æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤å½“å‰ç›®å½•ä¸‹çš„ filtered_tokens.txtï¼‰\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # æ‰“å¼€æ–‡ä»¶å¹¶å†™å…¥ï¼Œæ¯è¡Œä¸€ä¸ª token\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            for token in filtered_tokens:\n",
    "                f.write(f\"{token}\\n\")  # æ¯è¡Œå†™ä¸€ä¸ª tokenï¼Œä¾¿äºæŸ¥çœ‹\n",
    "        print(f\"âœ… filtered_tokens å·²ä¿å­˜åˆ°ï¼š{file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜å¤±è´¥ï¼š{str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01365f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è¯»å–æœ¬åœ°æ–‡ä»¶ï¼š./data/23950-0.txt\n",
      "æ–‡ä»¶ç¼–ç ï¼šutf-8ï¼ˆè¯»å–æˆåŠŸï¼‰\n",
      "v3: []\n"
     ]
    }
   ],
   "source": [
    "#ori_doc = load_file('./data/1342-0.txt')\n",
    "# processed_doc, lang = normalize_doc(ori_doc, isDebug=False)\n",
    "#processed_doc2 = load_file('./data/1342-0-p.txt')\n",
    "#lang = 'en'\n",
    "\n",
    "ori_doc = load_file('./data/23950-0.txt')\n",
    "\n",
    "test_v3 = test_stopwords(ori_doc, 'zh')\n",
    "print(f'v3: {test_v3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22b67717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg's Romance of the Three Kingdoms, by Guanzhong Luo\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: Romance of the Three Kingdoms\n",
      "\n",
      "Author: Guanzhong Luo\n",
      "\n",
      "Release Date: December 21, 2007 [EBook #23950]\n",
      "\n",
      "Language: Chinese\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK ROMANCE OF THE THREE KINGDOMS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by Jian-Lun Huang\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ç¬¬ä¸€å›ï¼šå®´æ¡ƒåœ’è±ªå‚‘ä¸‰çµç¾©ï¼Œæ–¬é»ƒå·¾è‹±é›„é¦–ç«‹åŠŸ\n",
      "\n",
      "ã€€ã€€è©æ›°ï¼š\n",
      "\n",
      "ã€€ã€€æ»¾æ»¾é•·æ±Ÿæ±é€æ°´ï¼ŒæµªèŠ±æ·˜ç›¡è‹±é›„ã€‚æ˜¯éæˆæ•—è½‰é ­ç©ºï¼šé’å±±ä¾èˆŠåœ¨ï¼Œå¹¾åº¦å¤•é™½ç´…ã€‚ç™½\n",
      "é«®æ¼æ¨µæ±Ÿæ¸šä¸Šï¼Œæ…£çœ‹ç§‹æœˆæ˜¥é¢¨ã€‚ä¸€å£ºæ¿é…’å–œç›¸é€¢ï¼šå¤ä»Šå¤šå°‘äº‹ï¼Œéƒ½ä»˜ç¬‘è«‡ä¸­ã€‚\n",
      "\n",
      "ã€€ã€€è©±èªªå¤©ä¸‹å¤§å‹¢ï¼Œåˆ†ä¹…å¿…åˆï¼Œåˆä¹…å¿…åˆ†ï¼šå‘¨æœ«ä¸ƒåœ‹åˆ†çˆ­ï¼Œå¹¶å…¥æ–¼ç§¦ã€‚åŠç§¦æ»…ä¹‹å¾Œï¼Œæ¥š\n",
      "ã€æ¼¢åˆ†çˆ­ï¼Œåˆå¹¶å…¥æ–¼æ¼¢ã€‚æ¼¢æœè‡ªé«˜ç¥–æ–¬ç™½è›‡è€Œèµ·ç¾©ï¼Œä¸€çµ±å¤©ä¸‹ã€‚å¾Œä¾†å…‰æ­¦ä¸­èˆˆï¼Œå‚³è‡³ç»\n",
      "å¸ï¼Œé‚åˆ†ç‚ºä¸‰åœ‹ã€‚æ¨å…¶è‡´äº‚ä¹‹ç”±ï¼Œæ®†å§‹æ–¼æ¡“ã€éˆäºŒå¸ã€‚æ¡“å¸ç¦éŒ®å–„é¡ï¼Œå´‡ä¿¡å®¦å®˜ã€‚åŠæ¡“\n",
      "å¸å´©ï¼Œéˆå¸å³ä½ï¼Œå¤§å°‡è»ç«‡æ­¦ã€å¤ªå‚…é™³è•ƒï¼Œå…±ç›¸è¼”ä½ã€‚æ™‚æœ‰å®¦å®˜æ›¹ç¯€ç­‰å¼„æ¬Šï¼Œç«‡æ­¦ã€é™³\n",
      "è•ƒè¬€èª…ä¹‹ï¼Œä½œäº‹ä¸å¯†ï¼Œåç‚ºæ‰€å®³ã€‚ä¸­æ¶“è‡ªæ­¤æ„ˆæ©«ã€‚\n",
      "\n",
      "ã€€ã€€å»ºå¯§äºŒå¹´å››æœˆæœ›æ—¥ï¼Œå¸å¾¡æº«å¾·æ®¿ã€‚æ–¹é™åº§ï¼Œæ®¿è§’ç‹‚é¢¨é©Ÿèµ·ï¼Œåªè¦‹ä¸€æ¢å¤§é’è›‡ï¼Œå¾æ¢\n",
      "ä¸Šé£›å°‡ä¸‹ä¾†ï¼ŒèŸ æ–¼æ¤…ä¸Šã€‚å¸é©šå€’ï¼Œå·¦å³æ€¥æ•‘å…¥å®®ï¼Œç™¾å®˜ä¿±å¥”é¿ã€‚é ˆè‡¾ï¼Œè›‡ä¸è¦‹äº†ã€‚å¿½ç„¶\n",
      "å¤§é›·å¤§é›¨ï¼ŒåŠ ä»¥å†°é›¹ï¼Œè½åˆ°åŠå¤œæ–¹æ­¢ï¼Œå£å»æˆ¿å±‹ç„¡æ•¸ã€‚å»ºå¯§å››å¹´äºŒæœˆï¼Œæ´›é™½åœ°éœ‡ï¼›åˆæµ·\n",
      "æ°´æ³›æº¢ï¼Œæ²¿æµ·å±…\n",
      "v4: []\n"
     ]
    }
   ],
   "source": [
    "print(ori_doc[:1000])\n",
    "'''\n",
    "processed_doc = cc_zh.convert(ori_doc)\n",
    "processed_doc = remove_special_characters(processed_doc, lang='zh')\n",
    "print(processed_doc[:1000])\n",
    "processed_doc = remove_stopwords(processed_doc, lang='zh')\n",
    "'''\n",
    "\n",
    "processed_doc, lang = normalize_doc(ori_doc, isDebug=False)\n",
    "test_v4 = test_stopwords(processed_doc, 'zh')\n",
    "\n",
    "print(f'v4: {test_v4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51fed1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_stopwords_in_file(token_file_path, lang='en'):\n",
    "    \"\"\"\n",
    "    ä» token æ–‡ä»¶ä¸­æ‰¾å‡ºæ‰€æœ‰å±äºåœç”¨è¯åˆ—è¡¨çš„ token\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        token_file_path (str): token æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ 'tmp_tokens.txt'ï¼‰\n",
    "        lang (str): è¯­è¨€ï¼Œ'en' ä¸ºè‹±æ–‡ï¼Œ'zh' ä¸ºä¸­æ–‡ï¼ˆé»˜è®¤è‹±æ–‡ï¼‰\n",
    "    è¿”å›ï¼š\n",
    "        list: å±äºåœç”¨è¯çš„ token åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # 2. åŠ è½½å¯¹åº”è¯­è¨€çš„åœç”¨è¯åˆ—è¡¨ï¼ˆè½¬ä¸ºé›†åˆï¼Œæé«˜æŸ¥è¯¢æ•ˆç‡ï¼‰\n",
    "    if lang == 'en':\n",
    "        stopword_set = set(nltk.corpus.stopwords.words('english'))\n",
    "    elif lang == 'zh':\n",
    "        stopword_set = set(nltk.corpus.stopwords.words('chinese'))\n",
    "    else:\n",
    "        raise ValueError(\"ä¸æ”¯æŒçš„è¯­è¨€ï¼Œä»…æ”¯æŒ 'en'ï¼ˆè‹±æ–‡ï¼‰æˆ– 'zh'ï¼ˆä¸­æ–‡ï¼‰\")\n",
    "    \n",
    "    # 3. è¯»å– token æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ª tokenï¼‰\n",
    "    if not os.path.exists(token_file_path):\n",
    "        raise FileNotFoundError(f\"æ–‡ä»¶ {token_file_path} ä¸å­˜åœ¨\")\n",
    "    \n",
    "    with open(token_file_path, 'r', encoding='utf-8') as f:\n",
    "        # è¯»å–æ‰€æœ‰è¡Œï¼Œå»é™¤æ¯è¡Œçš„æ¢è¡Œç¬¦å’Œå‰åç©ºæ ¼\n",
    "        tokens = [line.strip() for line in f.readlines() if line.strip()]  # è¿‡æ»¤ç©ºè¡Œ\n",
    "    \n",
    "    # 4. æ‰¾å‡ºå±äºåœç”¨è¯çš„ token\n",
    "    stopwords_in_file = [token for token in tokens if token in stopword_set]\n",
    "    \n",
    "    # 5. è¾“å‡ºç»“æœ\n",
    "    print(f\"åœ¨ {token_file_path} ä¸­æ‰¾åˆ° {len(stopwords_in_file)} ä¸ªåœç”¨è¯ï¼š\")\n",
    "    for idx, sw in enumerate(stopwords_in_file, 1):\n",
    "        print(f\"{idx}. {sw}\")\n",
    "    \n",
    "    # å¯é€‰ï¼šä¿å­˜ç»“æœåˆ°æ–‡ä»¶\n",
    "    output_file = f\"stopwords_in_{os.path.basename(token_file_path)}\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for sw in stopwords_in_file:\n",
    "            f.write(f\"{sw}\\n\")\n",
    "    print(f\"\\nç»“æœå·²ä¿å­˜åˆ°ï¼š{output_file}\")\n",
    "    \n",
    "    return stopwords_in_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8873aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åœ¨ tmp_token.txt ä¸­æ‰¾åˆ° 0 ä¸ªåœç”¨è¯ï¼š\n",
      "\n",
      "ç»“æœå·²ä¿å­˜åˆ°ï¼šstopwords_in_tmp_token.txt\n"
     ]
    }
   ],
   "source": [
    "token_file_path = 'tmp_token.txt'\n",
    "save_file = find_stopwords_in_file(token_file_path, lang='zh') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bfcc1347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(doc, lang='en'): \n",
    "    doc_ori = doc\n",
    "\n",
    "    print(f\"File1: {doc[:100]}...\\n{doc[len(doc)-100:]}\")\n",
    "    doc = strip_html_tags(doc) \n",
    "    print(f\"After strip_html_tags: {doc[:100]}\\n{doc[len(doc)-100:]}\")\n",
    "\n",
    "    if lang == 'en':\n",
    "        doc = remove_accented_chars(doc)\n",
    "        print(f\"After remove_accented_chars: {doc[:100]}\\n{doc[len(doc)-100:]}\")\n",
    "    elif lang == 'zh':\n",
    "        doc = cc_zh.convert(doc)\n",
    "        print(f\"ä¸­æ–‡ç¹ä½“è½¬æ¢ä¸ºç®€ä½“...{doc[:10]}...\")\n",
    "    else:\n",
    "        print(\"è­¦å‘Šï¼šæ— æ³•è¯†åˆ«æ–‡æ¡£è¯­è¨€ï¼Œè·³è¿‡é¢„å¤„ç†\")\n",
    "        return doc, 'unknown'\n",
    "    \n",
    "    doc = contractions.fix(doc) \n",
    "    print(f\"After contractions: {doc[:100]}\\n{doc[len(doc)-100:]}\")\n",
    "    doc_ori_count = doc_ori.count(\"'\")\n",
    "    doc_count = doc.count(\"'\")\n",
    "    print(f\"Check contractions: {doc_ori_count}, {doc_count}\")\n",
    "    print(test_english_contractions(doc_ori, doc))                                                                 \n",
    "\n",
    "    doc = doc.lower()\n",
    "    print(f\"After lower: {doc[:100]}\\n{doc[len(doc)-100:]}\")\n",
    "    \n",
    "    doc = lemmatize_text(doc)\n",
    "    print(f\"After lemmatization: {doc[:100]}\\n{doc[len(doc)-100:]}\")\n",
    "    \n",
    "    doc = remove_special_characters(doc, lang, remove_digits=True)\n",
    "    print(f\"After remove_special_characters: {doc[:100]}\\n{doc[len(doc)-100:]}\")    \n",
    "\n",
    "    # remove stopwords\n",
    "    doc = remove_stopwords(doc, is_lower_case=False)\n",
    "    print(f\"After remove_stopwords: {doc[:100]}\\n{doc[len(doc)-100:]}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c9ace762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File1: [Illustration:\n",
      "\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "\n",
      "...\n",
      "  CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "After strip_html_tags: [Illustration:\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "  \n",
      "  CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "After remove_accented_chars: [Illustration:\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "  \n",
      "  CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "After contractions: [Illustration:\n",
      "                             GEORGE ALLEN\n",
      "                               PUBLISHER\n",
      "  \n",
      "  CHISWICK PRESS:--CHARLES WHITTINGHAM AND CO.\n",
      "                  TOOKS COURT, CHANCERY LANE, LONDON.\n",
      "Check contractions: 0, 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test_english_contractions() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m      2\u001b[0m     lang \u001b[38;5;241m=\u001b[39m detect_language(doc)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtest_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 23\u001b[0m, in \u001b[0;36mtest_all\u001b[0;34m(doc, lang)\u001b[0m\n\u001b[1;32m     21\u001b[0m doc_count \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck contractions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_ori_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_english_contractions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_ori\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m)                                                                 \n\u001b[1;32m     25\u001b[0m doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter lower: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc[:\u001b[38;5;241m100\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc[\u001b[38;5;28mlen\u001b[39m(doc)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: test_english_contractions() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    lang = detect_language(doc)\n",
    "    test_all(doc, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f25aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docs[2]\n",
    "lang = detect_language(doc)\n",
    "test_all(doc, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babfe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "langs = []\n",
    "\n",
    "for doc in docs:        \n",
    "    processed_doc, lang = normalize_doc(doc, isDebug=False)\n",
    "    save_processed_text('./data/processed.txt', processed_doc)\n",
    "    processed_docs.append(processed_doc)\n",
    "    langs.append(lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d1ab4",
   "metadata": {},
   "source": [
    "# æµ‹è¯•æ¨¡å—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb208bd0",
   "metadata": {},
   "source": [
    "# ä¸»å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa59b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] config_file\n",
      "ipykernel_launcher.py: error: the following arguments are required: config_file\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hqyang/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(booklist):\n",
    "    print(\"# é¢„å¤„ç†ç»“æœæŠ¥å‘Š\\n\") \n",
    "    \n",
    "    for bookname, file_path in booklist:\n",
    "        print(f\"\\n##=== å¤„ç†ä¹¦ç±ï¼šã€Š{bookname}ã€‹===\")\n",
    "           \n",
    "        # 1. è·å–åŸå§‹æ–‡æœ¬\n",
    "        original_doc = fetch_gutenberg_text(file_path)\n",
    "        if original_doc is None:\n",
    "            print(f\"æ— æ³•è·å–æ–‡æœ¬ï¼Œè·³è¿‡å¤„ç†\")\n",
    "            continue    \n",
    "                \n",
    "        print(f\"æˆåŠŸè·å–æ–‡æœ¬:é•¿åº¦{len(original_doc)}å­—ç¬¦\")\n",
    "        \n",
    "        # 2. æ‰§è¡Œé¢„å¤„ç†\n",
    "        processed_doc, lang = normalize_doc(\n",
    "            original_doc,\n",
    "            html_stripping=True,\n",
    "            contraction_expansion=True,\n",
    "            accented_char_removal=True,\n",
    "            text_lower_case=True,\n",
    "            text_lemmatization=True,\n",
    "            special_char_removal=True,\n",
    "            stopword_removal=True,\n",
    "            remove_digits=True,\n",
    "            zh_simplification=True,\n",
    "            isDebug=False   \n",
    "        )\n",
    "\n",
    "        print(f\"é¢„å¤„ç†å®Œæˆï¼Œè¯­è¨€ï¼š{lang}ï¼Œå¤„ç†åé•¿åº¦ï¼š{len(processed_doc)}å­—ç¬¦\")\n",
    "           \n",
    "        if lang == 'unknown' or not processed_doc:\n",
    "            print(\"è·³è¿‡æ£€æŸ¥ï¼ˆæœªçŸ¥è¯­è¨€æˆ–ç©ºæ–‡æœ¬ï¼‰\")\n",
    "            continue\n",
    "\n",
    "        # 3. ç»“æœè¾“å‡ºå­˜å‚¨\n",
    "        save_processed_text(file_path, processed_doc)  # è°ƒç”¨ä¿å­˜å‡½æ•°\n",
    "\n",
    "        # 4. è¾“å‡ºå‰10é«˜é¢‘è¯åŠå‰20é•¿çš„å•è¯\n",
    "        n, k = 10, 20\n",
    "        top_n, longest_k = get_statistics(processed_doc, n=n, k=k, lang=lang)\n",
    "        print(f\"\\n=== è¾“å‡ºæ–‡æœ¬å‰{n}é«˜é¢‘è¯å’Œå‰{k}é•¿çš„å•è¯ ===\")\n",
    "        print(f\"å‰{n}é«˜é¢‘è¯ï¼š{top_n}\")\n",
    "        print(f\"å‰{k}é•¿å•è¯ï¼š{longest_k}\\n\")\n",
    "\n",
    "        print(f\"\\n=== æ£€æŸ¥é¢„å¤„ç†ç»“æœï¼šã€Š{bookname}ã€‹ ===\")            \n",
    "        \n",
    "        # è·å–åŸå§‹æ–‡æœ¬å’Œå¤„ç†åæ–‡æœ¬\n",
    "        errors = test_preprocessed_text(processed_doc, lang) if processed_doc else []\n",
    "        if not errors:\n",
    "            print(\"âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡\")\n",
    "        else:\n",
    "            print(\"âŒ é”™è¯¯ï¼š\")\n",
    "            for e in errors:\n",
    "                print(f\"- {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ä¿®æ”¹å‘½ä»¤è¡Œå‚æ•°è§£æï¼šæ¥æ”¶é…ç½®æ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°\n",
    "    parser = argparse.ArgumentParser(description=\"å¤„ç†ä¹¦ç±åˆ—è¡¨å¹¶ç”Ÿæˆç»“æœï¼ˆé…ç½®æ–‡ä»¶ä¸ºJSONæ ¼å¼ï¼‰\")\n",
    "    parser.add_argument('config_file', help=\"ä¹¦ç±åˆ—è¡¨é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ booklist.jsonï¼‰\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # åŠ¨æ€è·å–é…ç½®æ–‡ä»¶è·¯å¾„\n",
    "    config_path = Path(args.config_file)\n",
    "    \n",
    "    # éªŒè¯é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not config_path.exists():\n",
    "        print(f\"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ '{config_path}' ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„\")\n",
    "        exit(1)\n",
    "    \n",
    "    # è¯»å–å¹¶è§£æé…ç½®æ–‡ä»¶\n",
    "    try:\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        booklist = config.get('booklist', [])\n",
    "        if not booklist:\n",
    "            print(\"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ 'booklist' åˆ—è¡¨\")\n",
    "            exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ '{config_path}' æ ¼å¼é”™è¯¯ï¼ˆéœ€ä¸ºJSONæ ¼å¼ï¼‰\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"é”™è¯¯ï¼šè¯»å–é…ç½®æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸ - {str(e)}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # æ‰§è¡Œä¸»å‡½æ•°\n",
    "    main(booklist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -d Assign1-1-ans.py booklist_config.json  # -d è¡¨ç¤ºå¼€å¯è°ƒè¯•æ¨¡å¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
