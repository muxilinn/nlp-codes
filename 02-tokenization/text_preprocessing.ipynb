{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d374d7",
   "metadata": {},
   "source": [
    "# å®‰è£…ä¾èµ–å…³ç³» Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cf4c8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444757d",
   "metadata": {},
   "source": [
    "# å¤§å°å†™è½¬æ¢ Case Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20b7e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "        Natural Language Processing (NLP) is a fascinating field!\n",
    "        It involves teaching machines to understand human language.   \n",
    "        Let's learn the course, 1B00383, together!\n",
    "        ''' \n",
    "# text = text.strip()  # å»é™¤é¦–å°¾ç©ºç™½å­—ç¬¦ Strip leading and trailing whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "260031bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: \n",
      "        Natural Language Processing (NLP) is a fascinating field!\n",
      "        It involves teaching machines to understand human language.   \n",
      "        Let's learn the course, 1B00383, together!\n",
      "        \n",
      "lower: \n",
      "        natural language processing (nlp) is a fascinating field!\n",
      "        it involves teaching machines to understand human language.   \n",
      "        let's learn the course, 1b00383, together!\n",
      "        \n",
      "upper: \n",
      "        NATURAL LANGUAGE PROCESSING (NLP) IS A FASCINATING FIELD!\n",
      "        IT INVOLVES TEACHING MACHINES TO UNDERSTAND HUMAN LANGUAGE.   \n",
      "        LET'S LEARN THE COURSE, 1B00383, TOGETHER!\n",
      "        \n",
      "title: \n",
      "        Natural Language Processing (Nlp) Is A Fascinating Field!\n",
      "        It Involves Teaching Machines To Understand Human Language.   \n",
      "        Let'S Learn The Course, 1B00383, Together!\n",
      "        \n",
      "capitalize: \n",
      "        natural language processing (nlp) is a fascinating field!\n",
      "        it involves teaching machines to understand human language.   \n",
      "        let's learn the course, 1b00383, together!\n",
      "        \n",
      "swapcase: \n",
      "        nATURAL lANGUAGE pROCESSING (nlp) IS A FASCINATING FIELD!\n",
      "        iT INVOLVES TEACHING MACHINES TO UNDERSTAND HUMAN LANGUAGE.   \n",
      "        lET'S LEARN THE COURSE, 1b00383, TOGETHER!\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(f'original: {text}')\n",
    "print(f'lower: {text.lower()}')\n",
    "print(f'upper: {text.upper()}')\n",
    "print(f'title: {text.title()}')\n",
    "print(f'capitalize: {text.capitalize()}') \n",
    "# é¦–å­—æ¯å¤§å†™ Capitalize the first letter of the string\n",
    "# å› ä¸ºé¦–å­—æ¯å¯èƒ½æ˜¯æ¢è¡Œç¬¦å’Œç©ºæ ¼ï¼Œè€Œéå­—æ¯ï¼Œå¯¼è‡´ capitalize() çš„è¡Œä¸ºä¸ lower() éƒ¨åˆ†é‡åˆ\n",
    "print(f'swapcase: {text.swapcase()}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81e6da",
   "metadata": {},
   "source": [
    "# è¯è¯­åˆ‡è¯ Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_tokens:\n",
      " ['\\n        Natural Language Processing (NLP) is a fascinating field!', 'It involves teaching machines to understand human language.', \"Let's learn the course, 1B00383, together!\"]\n",
      "word_text:\n",
      " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '!', 'It', 'involves', 'teaching', 'machines', 'to', 'understand', 'human', 'language', '.', 'Let', \"'s\", 'learn', 'the', 'course', ',', '1B00383', ',', 'together', '!']\n",
      "split:\n",
      " ['Natural', 'Language', 'Processing', '(NLP)', 'is', 'a', 'fascinating', 'field!', 'It', 'involves', 'teaching', 'machines', 'to', 'understand', 'human', 'language.', \"Let's\", 'learn', 'the', 'course,', '1B00383,', 'together!']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "print(f'sent_tokens:\\n {sent_tokens}')\n",
    "\n",
    "word_text = nltk.word_tokenize(text)\n",
    "print(f'word_text:\\n {word_text}')\n",
    "\n",
    "print(f'split:\\n {text.split()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef1959",
   "metadata": {},
   "source": [
    "# åŠ è½½æ•°æ®é›† Load Datasets\n",
    "## ä¾‹å­1: Brown è¯­æ–™åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10910da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬ä¸€æ­¥ï¼šä¸‹è½½ Brown è¯­æ–™åº“ï¼ˆä»…éœ€ä¸‹è½½ä¸€æ¬¡ï¼‰\n",
    "nltk.download('brown')\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šåŠ è½½è¯­æ–™åº“\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14acb4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown è¯­æ–™åº“çš„æ–‡ä½“ç±»åˆ«ï¼š\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "\n",
      "æ–°é—»æ–‡ä½“å‰ 10 ä¸ªè¯ï¼š\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
      "\n",
      "æ–°é—»æ–‡ä½“ä¸­æœ€å¸¸è§çš„ 5 ä¸ªè¯ï¼š\n",
      "[('the', 5580), (',', 5188), ('.', 4030), ('of', 2849), ('and', 2146)]\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨è¯­æ–™åº“ï¼ˆç¤ºä¾‹æ“ä½œï¼‰\n",
    "# 1. æŸ¥çœ‹è¯­æ–™åº“åŒ…å«çš„æ–‡ä½“ç±»åˆ«\n",
    "print(\"Brown è¯­æ–™åº“çš„æ–‡ä½“ç±»åˆ«ï¼š\")\n",
    "print(brown.categories())  # è¾“å‡ºï¼š['adventure', 'belles_lettres', 'editorial', ...]\n",
    "\n",
    "# 2. è·å–æŸç±»æ–‡ä½“çš„æ–‡æœ¬ï¼ˆä»¥ \"news\" æ–°é—»æ–‡ä½“ä¸ºä¾‹ï¼‰\n",
    "news_text = brown.words(categories='news')\n",
    "print(\"\\næ–°é—»æ–‡ä½“å‰ 10 ä¸ªè¯ï¼š\")\n",
    "print(news_text[:10])  # è¾“å‡ºï¼š['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
    "\n",
    "# 3. ç»Ÿè®¡è¯é¢‘\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(news_text)\n",
    "print(\"\\næ–°é—»æ–‡ä½“ä¸­æœ€å¸¸è§çš„ 5 ä¸ªè¯ï¼š\")\n",
    "print(fdist.most_common(5))  # è¾“å‡ºï¼š[('the', 5580), (',', 5188), ('.', 4030), ...]\n",
    "\n",
    "# Visualize word frequencies\n",
    "fdist.plot(10, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ•°æ®é›† Load Datasets\n",
    "## ä¾‹å­2: Brown è¯­æ–™åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd3420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ç¬¬ä¸€æ­¥ï¼šä¸‹è½½ Penn Treebank è¯­æ–™åº“ï¼ˆä»…éœ€ä¸‹è½½ä¸€æ¬¡ï¼‰\n",
    "# æ³¨æ„ï¼šNLTK ä¸­çš„ Penn Treebank æ˜¯ç®€åŒ–ç‰ˆï¼ˆå«åå°”è¡—æ—¥æŠ¥ç­‰æ–‡æœ¬ï¼‰\n",
    "nltk.download('treebank')\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šåŠ è½½è¯­æ–™åº“\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf02ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯­æ–™åº“æ–‡ä»¶æ•°é‡ï¼š 199\n",
      "å‰ 5 ä¸ªæ–‡ä»¶ï¼š ['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', 'wsj_0005.mrg']\n",
      "\n",
      "ç¬¬ä¸€å¥çš„è¯æ€§æ ‡æ³¨ï¼š\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
      "\n",
      "ç¬¬ä¸€å¥çš„å¥æ³•ç»“æ„æ ‘ï¼š\n",
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸‰æ­¥ï¼šæŸ¥çœ‹è¯­æ–™åº“åŸºæœ¬ä¿¡æ¯\n",
    "# 1. æŸ¥çœ‹åŒ…å«çš„æ–‡ä»¶ï¼ˆæŒ‰å¥å­ç¼–å·å­˜å‚¨ï¼‰\n",
    "print(\"è¯­æ–™åº“æ–‡ä»¶æ•°é‡ï¼š\", len(treebank.fileids()))  # è¾“å‡ºï¼š199 ä¸ªæ–‡ä»¶\n",
    "print(\"å‰ 5 ä¸ªæ–‡ä»¶ï¼š\", treebank.fileids()[:5])  # å¦‚ ['wsj_0001.mrg', 'wsj_0002.mrg', ...]\n",
    "\n",
    "# 2. è·å–å¥å­çš„è¯æ€§æ ‡æ³¨ï¼ˆä»¥ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ç¬¬ä¸€å¥ä¸ºä¾‹ï¼‰\n",
    "tagged_sent = treebank.tagged_sents()[0]  # æ¯ä¸ªå¥å­æ˜¯ (è¯, è¯æ€§æ ‡ç­¾) çš„åˆ—è¡¨\n",
    "print(\"\\nç¬¬ä¸€å¥çš„è¯æ€§æ ‡æ³¨ï¼š\")\n",
    "print(tagged_sent)  # è¾“å‡ºï¼š[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
    "\n",
    "# 3. è·å–å¥æ³•ç»“æ„æ ‘ï¼ˆçŸ­è¯­ç»“æ„æ ‘ï¼‰\n",
    "parse_tree = treebank.parsed_sents()[0]  # å¥å­çš„å¥æ³•æ ‘å¯¹è±¡\n",
    "print(\"\\nç¬¬ä¸€å¥çš„å¥æ³•ç»“æ„æ ‘ï¼š\")\n",
    "print(parse_tree)  # è¾“å‡ºæ ‘çŠ¶ç»“æ„ï¼ˆå¦‚ (S (NP (NNP Pierre) (NNP Vinken)) ... )\n",
    "\n",
    "# 4. å¯è§†åŒ–å¥æ³•æ ‘ï¼ˆéœ€å®‰è£… matplotlib åº“ï¼‰\n",
    "parse_tree.draw()  # å¼¹å‡ºçª—å£æ˜¾ç¤ºæ ‘çŠ¶å›¾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb7f79",
   "metadata": {},
   "source": [
    "# 1. å»é™¤HTMLæ ‡ç­¾å’Œå™ªéŸ³ \n",
    "Removing HTML tags & noise\n",
    "# 2. åˆ é™¤é¢å¤–çš„ç©ºæ ¼å’Œæ¢è¡Œç¬¦ \n",
    "Remove extra whitespace and newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d342c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#pg-header #pg-machine-header p {\n",
      "    text-indent: -4em;\n",
      "    margin-left: 4em;\n",
      "    margin-top: 1em;\n",
      "    margin-bottom: 0;\n",
      "    font-size: medium\n",
      "}\n",
      "#pg-header #pg-header-authlist {\n",
      "    all: initial;\n",
      "    margin-top: 0;\n",
      "    margin-bottom: 0;\n",
      "}\n",
      "terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where yo\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# ä¸‹è½½ HTML æ ¼å¼çš„æ–‡æœ¬æ•°æ®: å¤è…¾å ¡è®¡åˆ’ï¼ˆProject Gutenbergï¼‰ \n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.text\n",
    "print(content[950:1200]) # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8358c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** START OF THE PROJECT GUTENBERG EBOOK THE BIBLE, KING JAMES VERSION, BOOK 1: GENESIS ***\n",
      "This eBook was produced by David Widger\n",
      "with the help of Derek Andrew's text from January 1992\n",
      "and the work of Bryan Taylor in November 2002.\n",
      "Book 01        Genesis\n",
      "01:001:001 In the beginning God created the heaven and the earth.\n",
      "01:001:002 And the earth was without form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_text = soup.get_text()\n",
    "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    return stripped_text\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[953:1452])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a770daac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTMLå†…å®¹å·²æˆåŠŸå­˜å‚¨åˆ° Chi_dataï¼Œæ–‡ä»¶åä¸ºï¼špg7337-images.html\n",
      "å†…å®¹é•¿åº¦ï¼š38766 å­—ç¬¦\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "# åˆå§‹åŒ–å˜é‡ï¼Œç”¨äºå­˜å‚¨HTMLå†…å®¹\n",
    "Chi_data = None\n",
    "\n",
    "# ä¸‹è½½ZIPæ–‡ä»¶\n",
    "url = 'https://www.gutenberg.org/cache/epub/7337/pg7337-h.zip'\n",
    "try:\n",
    "    response = requests.get(url, timeout=10)\n",
    "    response.raise_for_status()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"ä¸‹è½½å¤±è´¥ï¼š{e}\")\n",
    "else:\n",
    "    # è§£å‹å¹¶è¯»å–HTMLå†…å®¹\n",
    "    try:\n",
    "        with zipfile.ZipFile(BytesIO(response.content), 'r') as zip_ref:\n",
    "            file_list = zip_ref.namelist()\n",
    "            html_files = [f for f in file_list if f.endswith('.html')]\n",
    "            \n",
    "            if html_files:\n",
    "                # è¯»å–ç¬¬ä¸€ä¸ªHTMLæ–‡ä»¶å¹¶å­˜å‚¨åˆ°Chi_data\n",
    "                with zip_ref.open(html_files[0]) as html_file:\n",
    "                    Chi_data = html_file.read().decode('utf-8')  # å­˜å‚¨å®Œæ•´HTMLå†…å®¹\n",
    "                    print(f\"HTMLå†…å®¹å·²æˆåŠŸå­˜å‚¨åˆ° Chi_dataï¼Œæ–‡ä»¶åä¸ºï¼š{html_files[0]}\")\n",
    "                    print(f\"å†…å®¹é•¿åº¦ï¼š{len(Chi_data)} å­—ç¬¦\")\n",
    "            else:\n",
    "                print(\"ZIPä¸­æœªæ‰¾åˆ°HTMLæ–‡ä»¶ï¼ŒChi_dataä¿æŒä¸ºNone\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(\"æ— æ•ˆçš„ZIPæ–‡ä»¶ï¼ŒChi_dataä¿æŒä¸ºNone\")\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†æ–‡ä»¶å‡ºé”™ï¼š{e}ï¼ŒChi_dataä¿æŒä¸ºNone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8b663939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi_data å†…å®¹é¢„è§ˆï¼š\n",
      "BOOK é“å¾·ç¶“ ***</span>\n",
      "</div></section><p id=\"id00000\">Produced by Ching-yi Chen</p>\n",
      "\n",
      "<p id=\"id00001\" style=\"margin-top: 12em\">è€å­ã€Šé“å¾·ç¶“ã€‹ ç¬¬ä¸€~å››åç« </p>\n",
      "\n",
      "<p id=\"id00002\">è€å­é“ç¶“</p>\n",
      "\n",
      "<p id=\"id00003\">ç¬¬ä¸€ç« </p>\n",
      "\n",
      "<p id=\"id00004\">é“å¯é“ï¼Œéå¸¸é“ã€‚åå¯åï¼Œéå¸¸åã€‚ç„¡ï¼Œåå¤©åœ°ä¹‹å§‹ï¹”æœ‰ï¼Œåè¬ç‰©ä¹‹æ¯ã€‚\n",
      "æ•…å¸¸ç„¡ï¼Œæ¬²ä»¥è§€å…¶å¦™ï¼›å¸¸æœ‰ï¼Œæ¬²ä»¥è§€å…¶å¾¼ã€‚æ­¤å…©è€…ï¼ŒåŒå‡ºè€Œç•°åï¼ŒåŒè¬‚ä¹‹\n",
      "ç„ã€‚ç„ä¹‹åˆç„ï¼Œçœ¾å¦™ä¹‹é–€ã€‚\n"
     ]
    }
   ],
   "source": [
    "# åç»­å¯ç›´æ¥ä½¿ç”¨ Chi_data å˜é‡ï¼ˆä¾‹å¦‚æ‰“å°å‰1000å­—ç¬¦ï¼‰\n",
    "if Chi_data is not None:\n",
    "    print(\"\\nChi_data å†…å®¹é¢„è§ˆï¼š\")\n",
    "    print(Chi_data[7500:7800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "02a975b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: é“å¾·ç¶“\n",
      "Author: Laozi\n",
      "Release date: January 1, 2005 [eBook #7337]\n",
      "                Most recently updated: December 30, 2020\n",
      "Language: Chinese\n",
      "Credits: Produced by Ching-yi Chen\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK é“å¾·ç¶“ ***\n",
      "Produced by Ching-yi Chen\n",
      "è€å­ã€Šé“å¾·ç¶“ã€‹ ç¬¬ä¸€~å››åç« \n",
      "è€å­é“ç¶“\n",
      "ç¬¬ä¸€ç« \n",
      "é“å¯é“ï¼Œéå¸¸é“ã€‚åå¯åï¼Œéå¸¸åã€‚ç„¡ï¼Œåå¤©åœ°ä¹‹å§‹ï¹”æœ‰ï¼Œåè¬ç‰©ä¹‹æ¯ã€‚\n",
      "æ•…å¸¸ç„¡ï¼Œæ¬²ä»¥è§€å…¶å¦™ï¼›å¸¸æœ‰ï¼Œæ¬²ä»¥è§€å…¶å¾¼ã€‚æ­¤å…©è€…ï¼ŒåŒå‡ºè€Œç•°åï¼ŒåŒè¬‚ä¹‹\n",
      "ç„ã€‚ç„ä¹‹åˆç„ï¼Œçœ¾å¦™ä¹‹é–€ã€‚\n",
      "ç¬¬äºŒç« \n",
      "å¤©ä¸‹çš†çŸ¥ç¾ä¹‹ç‚ºç¾ï¼Œæ–¯æƒ¡çŸ£ï¹”çš†çŸ¥å–„ä¹‹ç‚ºå–„ï¼Œæ–¯ä¸å–„çŸ£ã€‚æ•…æœ‰ç„¡ç›¸ç”Ÿï¼Œé›£\n",
      "æ˜“ç›¸æˆï¼Œé•·çŸ­ç›¸å½¢ï¼Œé«˜ä¸‹ç›¸å‚¾ï¼ŒéŸ³è²ç›¸å’Œï¼Œå‰å¾Œç›¸éš¨ã€‚æ˜¯ä»¥è–äººè™•ã€Œç„¡ç‚º\n",
      "ã€ä¹‹äº‹ï¼Œè¡Œã€Œä¸è¨€ã€ä¹‹æ•™ã€‚è¬ç‰©ä½œç„‰è€Œä¸è¾­ï¼Œç”Ÿè€Œä¸æœ‰ï¼Œç‚ºè€Œä¸æƒï¼ŒåŠŸæˆ\n",
      "è€Œå¼—å±…ã€‚å¤«å”¯å¼—å±…ï¼Œæ˜¯ä»¥ä¸å»ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_chi_content = strip_html_tags(Chi_data)\n",
    "print(clean_chi_content[524:999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbc20f",
   "metadata": {},
   "source": [
    "# 3. åˆ é™¤ç‰¹æ®Šå­—ç¬¦(é‡è¯») Removing Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1cd29547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a791a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt\n",
      "Accented_processed: Some Accented text\n"
     ]
    }
   ],
   "source": [
    "s = 'SÃ³mÄ› ÃccÄ›ntÄ›d tÄ›xt'\n",
    "print(f'Original: {s}')   \n",
    "print(f'Accented_processed: {remove_accented_chars(s)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb3cfe",
   "metadata": {},
   "source": [
    "## ä¸­æ–‡å¤„ç†\n",
    "- â€œç®€ä½“â€\n",
    "- â€œç¹ä½“â€\n",
    "- â€œè‡ºç£æ­£é«”â€\n",
    "- â€œé¦™æ¸¯ç¹é«”â€\n",
    "- ç®€ç¹åˆ†æ­§è¯\n",
    "- ä¸€ç®€å¯¹å¤šç¹\n",
    "- ä¸€ç¹å¯¹å¤šç®€\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4a82713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencc-python-reimplemented\n",
      "  Obtaining dependency information for opencc-python-reimplemented from https://files.pythonhosted.org/packages/30/6b/055b7806f320cc8f2cdf23c5f70221c0dc1683fca9ffaf76dfc2ad4b91b6/opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading opencc_python_reimplemented-0.1.7-py2.py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m481.8/481.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: opencc-python-reimplemented\n",
      "Successfully installed opencc-python-reimplemented-0.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip install opencc-python-reimplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "467fbe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹ç¹ä½“æ–‡æœ¬ï¼š ä¸­è¯äººæ°‘å…±å’Œåœ‹ï¼Œè‡ºç£æ˜¯ä¸­åœ‹çš„ä¸€éƒ¨åˆ†ã€‚\n",
      "è½¬æ¢åç®€ä½“æ–‡æœ¬ï¼š ä¸­åäººæ°‘å…±å’Œå›½ï¼Œå°æ¹¾æ˜¯ä¸­å›½çš„ä¸€éƒ¨åˆ†ã€‚\n",
      "ç®€ä½“è½¬ç¹ä½“ï¼š æˆ‘æ„›ä¸­åœ‹ï¼Œæˆ‘æ„›åŒ—äº¬å¤©å®‰é–€ã€‚\n"
     ]
    }
   ],
   "source": [
    "from opencc import OpenCC\n",
    "\n",
    "# åˆå§‹åŒ–è½¬æ¢å™¨ï¼š\n",
    "# 't2s' è¡¨ç¤ºç¹ä½“è½¬ç®€ä½“ï¼ˆtraditional to simplifiedï¼‰\n",
    "# 's2t' è¡¨ç¤ºç®€ä½“è½¬ç¹ä½“ï¼ˆsimplified to traditionalï¼‰\n",
    "cc = OpenCC('t2s')  # ç¹ä½“è½¬ç®€ä½“\n",
    "\n",
    "traditional_text = \"ä¸­è¯äººæ°‘å…±å’Œåœ‹ï¼Œè‡ºç£æ˜¯ä¸­åœ‹çš„ä¸€éƒ¨åˆ†ã€‚\"\n",
    "print(\"åŸå§‹ç¹ä½“æ–‡æœ¬ï¼š\", traditional_text)\n",
    "\n",
    "# ç¹è½¬ç®€\n",
    "simplified_text = cc.convert(traditional_text)\n",
    "print(\"è½¬æ¢åç®€ä½“æ–‡æœ¬ï¼š\", simplified_text)  # è¾“å‡ºï¼š\"ä¸­åäººæ°‘å…±å’Œå›½ï¼Œå°æ¹¾æ˜¯ä¸­å›½çš„ä¸€éƒ¨åˆ†ã€‚\"\n",
    "\n",
    "\n",
    "cc_s2t = OpenCC('s2t')\n",
    "traditional_text2 = cc_s2t.convert(\"æˆ‘çˆ±ä¸­å›½ï¼Œæˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨ã€‚\")\n",
    "print(\"ç®€ä½“è½¬ç¹ä½“ï¼š\", traditional_text2)  # è¾“å‡ºï¼š\"æˆ‘æ„›ä¸­åœ‹ï¼Œæˆ‘æ„›åŒ—äº¬å¤©å®‰é–€ã€‚\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa58d4e",
   "metadata": {},
   "source": [
    "# 4. åˆ é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œæ•°å­—å’Œç¬¦å· \n",
    "Removing Special Characters, Numbers and Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eb85fc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\t Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ ğŸ™‚ğŸ™‚ğŸ™‚\n",
      "Special_char_processed:\n",
      "\t Well this was fun See you at 730 What do you think 9318 \n",
      "Special_char_and_digit_processed:\n",
      "\t Well this was fun See you at  What do you think  \n"
     ]
    }
   ],
   "source": [
    "s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ ğŸ™‚ğŸ™‚ğŸ™‚\"\n",
    "print(f'Original:\\n\\t {s}')\n",
    "print(f'Special_char_processed:\\n\\t {remove_special_characters(s)}')\n",
    "print(f'Special_char_and_digit_processed:\\n\\t {remove_special_characters(s, remove_digits=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f682a5",
   "metadata": {},
   "source": [
    "## Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "12ba086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Obtaining dependency information for contractions from https://files.pythonhosted.org/packages/bb/e4/725241b788963b460ce0118bfd5c505dd3d1bdd020ee740f9f39044ed4a7/contractions-0.1.73-py2.py3-none-any.whl.metadata\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Obtaining dependency information for textsearch>=0.0.21 from https://files.pythonhosted.org/packages/e2/0f/6f08dd89e9d71380a369b1f5b6c97a32d62fc9cfacc1c5b8329505b9e495/textsearch-0.0.24-py2.py3-none-any.whl.metadata\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Obtaining dependency information for anyascii from https://files.pythonhosted.org/packages/c2/76/783b75a21ce3563b8709050de030ae253853b147bd52e141edc1025aa268/anyascii-0.3.3-py3-none-any.whl.metadata\n",
      "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Obtaining dependency information for pyahocorasick from https://files.pythonhosted.org/packages/d1/6d/718db0b31ce5df316abadc01dd9e81e4a179ebbcb15c18f87c5d99bf7512/pyahocorasick-2.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pyahocorasick-2.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyahocorasick-2.2.0-cp39-cp39-macosx_11_0_arm64.whl (33 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n",
      "Requirement already satisfied: textsearch in /Users/hqyang/anaconda3/envs/py39/lib/python3.9/site-packages (0.0.24)\n",
      "Requirement already satisfied: anyascii in /Users/hqyang/anaconda3/envs/py39/lib/python3.9/site-packages (from textsearch) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in /Users/hqyang/anaconda3/envs/py39/lib/python3.9/site-packages (from textsearch) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install textsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6ff9d1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I'm\", 'I am'),\n",
       " (\"I'm'a\", 'I am about to'),\n",
       " (\"I'm'o\", 'I am going to'),\n",
       " (\"I've\", 'I have'),\n",
       " (\"I'll\", 'I will'),\n",
       " (\"I'll've\", 'I will have'),\n",
       " (\"I'd\", 'I would'),\n",
       " (\"I'd've\", 'I would have'),\n",
       " ('Whatcha', 'What are you'),\n",
       " (\"amn't\", 'am not')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "list(contractions.contractions_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ce26c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\n",
      "Expanded: You all cannot expand contractions I would think! You would not be able to. How did you do it?\n"
     ]
    }
   ],
   "source": [
    "s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\n",
    "\n",
    "print(\"Original:\", s)\n",
    "print(\"Expanded:\", contractions.fix(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe96b7a",
   "metadata": {},
   "source": [
    "# 5. è¯å¹²æå– æˆ– è¯å½¢è¿˜åŸ\n",
    "Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a1e67938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex1:\n",
      " Original: jumping jumps jumped\n",
      " Stemmed: ['jump', 'jump', 'jump']\n",
      "Ex2:\n",
      " Original: lying\n",
      " Stemmed: lie\n",
      "Ex3:\n",
      " Original: strange\n",
      " Stemmed: strang\n",
      "Ex4:\n",
      " Original: Interesting\n",
      " Stemmed: interest\n",
      "Ex5:\n",
      " Original: Interestingly\n",
      " Stemmed: interestingli\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print('Ex1:')\n",
    "s = 'jumping jumps jumped'\n",
    "print(f' Original: {s}')\n",
    "print(f' Stemmed: {[ps.stem(word) for word in s.split()]}')\n",
    "\n",
    "print('Ex2:')\n",
    "s = \"lying\"\n",
    "print(f' Original: {s}')\n",
    "print(f' Stemmed: {ps.stem(s)}')\n",
    "\n",
    "print('Ex3:')\n",
    "s = \"strange\"\n",
    "print(f' Original: {s}')\n",
    "print(f' Stemmed: {ps.stem(s)}')\n",
    "\n",
    "print('Ex4:')\n",
    "s = \"Interesting\"\n",
    "print(f' Original: {s}')\n",
    "print(f' Stemmed: {ps.stem(s)}')\n",
    "\n",
    "print('Ex5:')\n",
    "s = \"Interestingly\"\n",
    "print(f' Original: {s}')\n",
    "print(f' Stemmed: {ps.stem(s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "52ab7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmatize in module nltk.stem.wordnet:\n",
      "\n",
      "lemmatize(word: str, pos: str = 'n') -> str method of nltk.stem.wordnet.WordNetLemmatizer instance\n",
      "    Lemmatize `word` using WordNet's built-in morphy function.\n",
      "    Returns the input word unchanged if it cannot be found in WordNet.\n",
      "    \n",
      "    :param word: The input word to lemmatize.\n",
      "    :type word: str\n",
      "    :param pos: The Part Of Speech tag. Valid options are `\"n\"` for nouns,\n",
      "        `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
      "        for satellite adjectives.\n",
      "    :param pos: str\n",
      "    :return: The lemma of `word`, for the given `pos`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "help(wnl.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "70c521dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "box\n"
     ]
    }
   ],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('boxes', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3211e2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "44495679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5bfd20b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))\n",
    "print(wnl.lemmatize('fancier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "106b46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b5bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " ['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n",
      "L_d:\n",
      " ['The', 'brown', 'fox', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dog', '!']\n",
      "L_n:\n",
      " ['The', 'brown', 'fox', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dog', '!']\n",
      "L_v:\n",
      " ['The', 'brown', 'fox', 'be', 'quick', 'and', 'they', 'be', 'jump', 'over', 'the', 'sleep', 'lazy', 'dog', '!']\n",
      "L_a:\n",
      " ['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n",
      "L_r:\n",
      " ['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(s)\n",
    "\n",
    "print(f'Original:\\n {tokens}')\n",
    "print(f'L_d:\\n {[wnl.lemmatize(t) for t in tokens]}')  # é»˜è®¤è¯æ€§ä¸ºåè¯\n",
    "print(f\"L_n:\\n {[wnl.lemmatize(t, 'n') for t in tokens]}\")  # åè¯\n",
    "print(f\"L_v:\\n {[wnl.lemmatize(t, 'v') for t in tokens]}\")  # åŠ¨è¯\n",
    "print(f\"L_a:\\n {[wnl.lemmatize(t, 'a') for t in tokens]}\")  # å½¢å®¹è¯\n",
    "print(f\"L_r:\\n {[wnl.lemmatize(t, 'r') for t in tokens]}\")  # å‰¯è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c9c3fa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', 'JJ'), ('foxes', 'NNS'), ('are', 'VBP'), ('quick', 'JJ'), ('and', 'CC'), ('they', 'PRP'), ('are', 'VBP'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('sleeping', 'VBG'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "# è¯æ€§æ ‡æ³¨ Part-of-Speech (POS) Tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bc6067d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag conversion to WordNet Tags\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def pos_tag_wordnet(tagged_tokens):\n",
    "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
    "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
    "                            for word, tag in tagged_tokens]\n",
    "    return new_tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0c24dc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'n'), ('brown', 'a'), ('foxes', 'n'), ('are', 'v'), ('quick', 'a'), ('and', 'n'), ('they', 'n'), ('are', 'v'), ('jumping', 'v'), ('over', 'n'), ('the', 'n'), ('sleeping', 'v'), ('lazy', 'a'), ('dogs', 'n'), ('!', 'n')]\n",
      "Original:\n",
      " ['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n",
      "L_d:\n",
      " ['The', 'brown', 'fox', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dog', '!']\n",
      "L_n:\n",
      " ['The', 'brown', 'fox', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dog', '!']\n",
      "L_v:\n",
      " ['The', 'brown', 'fox', 'be', 'quick', 'and', 'they', 'be', 'jump', 'over', 'the', 'sleep', 'lazy', 'dog', '!']\n",
      "L_a:\n",
      " ['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n",
      "L_r:\n",
      " ['The', 'brown', 'foxes', 'are', 'quick', 'and', 'they', 'are', 'jumping', 'over', 'the', 'sleeping', 'lazy', 'dogs', '!']\n",
      "Lemmatization after WordNet Tags:\n",
      " ['The', 'brown', 'fox', 'be', 'quick', 'and', 'they', 'be', 'jump', 'over', 'the', 'sleep', 'lazy', 'dog', '!']\n"
     ]
    }
   ],
   "source": [
    "wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
    "print(wordnet_tokens)\n",
    "\n",
    "lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
    "\n",
    "print(f'Original:\\n {tokens}')\n",
    "print(f'L_d:\\n {[wnl.lemmatize(t) for t in tokens]}')  # é»˜è®¤è¯æ€§ä¸ºåè¯\n",
    "print(f\"L_n:\\n {[wnl.lemmatize(t, 'n') for t in tokens]}\")  # åè¯\n",
    "print(f\"L_v:\\n {[wnl.lemmatize(t, 'v') for t in tokens]}\")  # åŠ¨è¯\n",
    "print(f\"L_a:\\n {[wnl.lemmatize(t, 'a') for t in tokens]}\")  # å½¢å®¹è¯\n",
    "print(f\"L_r:\\n {[wnl.lemmatize(t, 'r') for t in tokens]}\")  # å‰¯è¯\n",
    "print(f'Lemmatization after WordNet Tags:\\n {[wnl.lemmatize(word, tag) for word, tag in wordnet_tokens]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d344a1",
   "metadata": {},
   "source": [
    "# 6. åˆ é™¤åœæ­¢è¯ \n",
    "Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d0688532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hqyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import jieba # ä¸­æ–‡åˆ†è¯å·¥å…·\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3a91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words in English:\n",
      "198\n",
      "{'more', 'few', 'its', 'now', 'not', 'nor', 'further', 'what', \"weren't\", 'very', 'while', \"they've\", 'been', 'haven', 't', 'd', 'he', 'by', 'have', 'aren', 'own', \"you've\", 'these', 'will', 'should', \"we're\", 'be', 'most', 'we', 'all', 'or', \"you'd\", 'themselves', \"won't\", 'isn', 'but', 'was', \"it's\", 'during', 'where', 'those', 'of', 'shan', 'with', \"it'd\", \"it'll\", 'just', 'on', 'as', 'being', 'at', \"we'd\", 'their', \"didn't\", \"they'll\", 'weren', 'yourself', \"haven't\", 'there', 'her', 'am', 'an', \"she'll\", 'yours', 'yourselves', \"aren't\", 's', 'who', 'shouldn', 're', 'off', 'o', \"don't\", 'myself', 'why', 'do', \"we'll\", \"they'd\", 'which', 'were', \"you'll\", \"couldn't\", 'too', 'mightn', 'don', 'is', \"she'd\", 'it', 'they', 'doesn', 'in', 'then', 'them', \"she's\", 'if', 'because', 'your', 'she', 'didn', \"mightn't\", 'to', \"that'll\", \"they're\", \"wasn't\", 'the', 'when', 'down', \"hasn't\", 'same', 'wasn', 'needn', 'both', 'are', \"i've\", 'wouldn', \"shan't\", 'you', 'll', 'can', 'ain', 'each', \"you're\", 'itself', 'once', 'into', \"wouldn't\", 'again', \"we've\", 'from', 'other', 'so', \"should've\", 'how', 'hers', 'did', 'ourselves', \"shouldn't\", 'here', 'before', 'herself', 'hasn', 'only', \"i'll\", 'theirs', 'over', \"hadn't\", \"needn't\", 'against', 'him', 'between', 'has', 'until', 'himself', 'his', 'had', 'about', 've', 'y', 'doing', 'whom', \"he's\", 'for', 'i', 'no', 'm', 'couldn', 'a', 'below', 'having', \"he'd\", 'this', 'through', 'does', \"he'll\", 'mustn', 'up', \"i'd\", \"mustn't\", \"i'm\", 'after', 'out', 'won', \"isn't\", 'our', 'some', 'ours', 'than', 'my', 'and', 'me', 'such', 'any', 'above', 'ma', 'under', 'hadn', \"doesn't\", 'that'}\n",
      "Stop words in Chinese:\n",
      "841\n",
      "{'ç›¸å', 'è®¤ä¸º', 'æ€ä¹ˆ', 'å®‰å…¨', 'ç»“æœ', 'æœ‰ç€', 'åŠå…¶', 'ä»€ä¹ˆæ ·', 'èµ·æ¥', 'æœ‰æ‰€', 'ä¸€ç‰‡', 'å¯èƒ½', 'å…ˆå', 'å±äº', 'é™¤äº†', 'è¿™ç‚¹', 'å¹¶æ²¡æœ‰', 'æ¥è‘—', 'åä¹‹', 'æ„æ€', 'é‚£', 'äº†è§£', 'å½’', 'é™¤', 'åŸºæœ¬', 'ä¿º', 'å®ƒä»¬', 'ç«‹å³', 'è¯´æ˜', 'å²‚ä½†', 'å¤æ‚', 'å³ä¾¿', 'æ—¢æ˜¯', 'æ¯«ä¸', 'è¦ä¹ˆ', 'å¾€å¾€', 'è¶ç€', 'è¾ƒ', 'æ¥ç€', 'é‡Œé¢', 'å®å’š', 'æ€', 'å½“æ—¶', 'é€‚ç”¨', 'å“‡', 'ä¿ƒè¿›', 'è¾¹', 'å€˜è‹¥', 'å˜', 'æ‰€è°“', 'ä¼Ÿå¤§', 'ä½ çš„', 'ä¸', 'å¦ä¸€æ–¹é¢', 'åˆ«', 'å¤§æ‰¹', 'å·²ç»', 'è€Œè¨€', 'å—¬', 'å®ƒ', 'å®è‚¯', 'è¡¨æ˜', 'è¿‡', 'ç°åœ¨', 'ä¸æƒŸ', 'äººå®¶', 'å…±åŒ', 'æ˜æ˜¾', 'é‚£éº½', 'å…¨éƒ¨', 'å¹¶ä¸”', 'ä»¬', 'å…¶ä¸­', 'ä¸å•', 'æ€æ ·', 'é™¤æ­¤ä¹‹å¤–', 'æ‰€ä»¥', 'ä¹‹å¾Œ', 'èƒ½', 'æ˜¾è‘—', 'ä¹‹å‰', 'ä»–', 'è‡ªä»', 'ä»–çš„', 'é€šå¸¸', 'çŸ¥é“', 'å¦‚è‹¥', 'ç”šè‡³', 'å¹¶ä¸', 'è¿˜æœ‰', 'æ­¤', 'ä¹‹', 'è¿™ä¹ˆ', 'æ¯å½“', 'å¥½è±¡', 'æˆä¸º', 'è«è‹¥', 'å¦‚æœ', 'ä»»å‡­', 'è¿™ä¹ˆæ ·', 'ä¸€', 'è¿™è¾¹', 'åŠ ä¹‹', 'æœ', 'å¸®åŠ©', 'æ™®é€š', 'æ¬¢è¿', 'å€˜ä½¿', 'æ„¿æ„', 'ä»»ä½•', 'è¿™å„¿', 'ä¸Šä¸‹', 'ä¸€åˆ‡', 'ä»¥å¤–', 'å†’', 'å¾—å‡º', 'æ‚¨', 'åæ˜ ', 'å…¶å®ƒ', 'çœ‹çœ‹', 'å¦å¤–', 'åšå†³', 'è¦æ˜¯', 'ä»¥è‡´', 'å³ä½¿', 'å®å¯', 'å®Œæˆ', 'æ¢è¨€ä¹‹', 'è¿›è¡Œ', 'æŒ‰ç…§', 'ä¸Šå‡', 'ä»Šå¹´', 'å‡ä½¿', 'è™½', 'ä¹Œä¹', 'é ', 'ä»–äºº', 'åˆ', 'æ…¢è¯´', 'èŒƒå›´', 'å“ˆ', 'ä¸ªåˆ«', 'æ€»æ˜¯', 'å“ªæ€•', 'ä¹', 'ç¡®å®š', 'çºµä»¤', 'è‹¥é', 'å……åˆ†', 'å¹¿å¤§', 'æˆ‘ä»¬', 'å°½', 'ç»™', 'é™åˆ¶', 'ä»€éº½', 'ä½ ä»¬', 'é€‚åº”', 'å°½ç®¡', 'å«åš', 'é„™äºº', 'ç»å¯¹', 'ç›´åˆ°', 'å†²', 'ä»Šå¾Œ', 'æ•´ä¸ª', 'åŠ å¼º', 'å§å“’', 'ä»¥ä¸‹', 'çœå¾—', 'ç­‰ç­‰', 'å¯¹', 'é¦–å…ˆ', 'ä¼¼çš„', 'å¹¿æ³›', 'ä»¥åŠ', 'å·¨å¤§', 'å¼€å±•', 'æ¯ä¸ª', 'çš„', 'ä¸€äº›', 'å·©å›º', 'å§', 'ç»å¸¸', 'ä½•å†µ', 'çºµ', 'å‘€', 'çŸ£', 'å…ˆç”Ÿ', 'ç»“åˆ', 'å…¨é¢', 'å…®', 'åŠ ä»¥', 'è¾¾åˆ°', 'ç‰¹æ®Š', 'éä½†', 'å¸¸å¸¸', 'è¿™é‡Œ', 'æ¢å¥è¯è¯´', 'ä¸»å¼ ', 'å¯è§', 'ä¹‹æ‰€ä»¥', 'æ€»çš„æ¥è¯´', 'ç€', 'å…·æœ‰', 'ä¸ä¼š', 'æ¯”å¦‚', 'ä¸ª', 'æ‰€', 'é˜²æ­¢', 'å„äºº', 'äºæ˜¯ä¹', 'ä¸Šè¿°', 'æ®', 'æ•…æ­¤', 'æ­¤æ—¶', 'è§‰å¾—', 'è¿™', 'ä»£æ›¿', 'æœ‰è‘—', 'ç„¶è€Œ', 'å‰é¢', 'ä¸æ˜¯', 'åˆ™', 'ä¸€æ–¹é¢', 'å¤§åŠ›', 'å›ºç„¶', 'ç°ä»£', 'äº', 'ä¸ç‹¬', 'åšæŒ', 'å†³å®š', 'æ­£å¸¸', 'ä½ ', 'è¿', 'éå¾’', 'å®', 'ç½¢äº†', 'ä½¿ç”¨', 'å‘—', 'ä¸ºä»€ä¹ˆ', 'é‰´äº', 'ä¸ç„¶', 'å…·ä½“', 'ä¸“é—¨', 'æœ€å¥½', 'è¿™å°±æ˜¯è¯´', 'å°šä¸”', 'è‡ªå„å„¿', 'ä»¥', 'æœ‰æ•ˆ', 'æœçœŸ', 'åŒæ—¶', 'è¢«', 'å“ªè¾¹', 'é«˜å…´', 'å‡ ä¹', 'å“', 'é€‚å½“', 'ç­‰', 'æˆ–', 'æ­¤é—´', 'ç®¡', 'ç»„æˆ', 'å†è¯´', 'ç»', 'è€…', 'é‚£å„¿', 'ä¸è¶³', 'æ¯', 'ä»Šå', 'ä¸­é—´', 'å…¶ä¸€', 'æ¯”æ–¹', 'å–å¾—', 'ä¸ºä»€éº½', 'åƒ', 'è½¬åŠ¨', 'å“ªä¸ª', 'å“ªå¹´', 'é¡º', 'ç¬¬', 'é‚£ä¹ˆ', 'æœ¬', 'ä¸å¤Ÿ', 'æ¯”', 'å¼ºè°ƒ', 'ç”±æ­¤å¯è§', 'åªæ˜¯', 'è¿›å…¥', 'å¾Œæ¥', 'ä¿ºä»¬', 'æœ€å', 'å“ªé‡Œ', 'å‡å¦‚', 'å„çº§', 'å’‹', 'ä¸»è¦', 'å‡ºå»', 'å¤±å»', 'å’±ä»¬', 'ç´§æ¥ç€', 'é¿å…', 'ä¹‹ç±»', 'æ˜¾ç„¶', 'ä¸‹æ¥', 'å¤„ç†', 'ä¸é—®', 'é—®é¢˜', 'å¤§çº¦', 'èµ·', 'åŠæ—¶', 'åé¢', 'è¿›æ­¥', 'æœ€å¤§', 'å¯', 'ç›¸åº”', 'å½¢æˆ', 'ä»€ä¹ˆ', 'é‚£é‡Œ', 'å—¯', 'æ ¹æ®', 'æ‰“', 'è¦æ±‚', 'è¿™æ ·', 'å¦‚ä¸‹', 'è¦', 'è¡ŒåŠ¨', 'è¿™æ—¶', 'æŸäº›', 'è¶', 'é‡æ–°', 'ä¸ä¸€', 'ä¸æ–­', 'è¯¥', 'ååˆ†', 'å„ä¸ª', 'æ²¿ç€', 'è‹¥', 'å½“', 'è€Œå¤–', 'åè¿‡æ¥è¯´', 'ä¸è¦', 'å’š', 'æŸ', 'å‡­å€Ÿ', 'ä¸€ç›´', 'ä¸‹åˆ—', 'åˆ«çš„', 'æŒæ¡', 'ä¸å¦', 'æ¯å¤©', 'ä¸ºç€', 'ä¸€å®š', 'å¤§é‡', 'åœ¨ä¸‹', 'ä¹Ÿå¥½', 'æ›´åŠ ', 'å‡è‹¥', 'ä¸æ‹˜', 'å¾—åˆ°', 'è€Œæ˜¯', 'ä¾', 'èµ¶', 'åœ¨', 'å¯¹åº”', 'å‘¼å“§', 'é‚£ä¹ˆäº›', 'é‚£ä¼šå„¿', 'å—åˆ°', 'é‡å¤§', 'ç‰¹ç‚¹', 'å³', 'ä¸Šé¢', 'è°', 'ä¸åª', 'å’Œ', 'å®Œå…¨', 'æˆ–è€…', 'æ•…', 'æœç€', 'å˜æˆ', 'ç”¨', 'ç›¸ä¿¡', 'å¦åˆ™', 'ä¸å˜', 'æ°æ°ç›¸å', 'äº‘äº‘', 'æ¥', 'ä¸å…‰', 'äº§ç”Ÿ', 'æ‰©å¤§', 'è‡ªä¸ªå„¿', 'ä¹Ÿç½¢', 'ä»¥ä¸º', 'å…¶', 'é‚£è¾¹', 'äººä»¬', 'å¹¶', 'ä¸°å¯Œ', 'çºµç„¶', 'è€Œå·²', 'æ³¨æ„', 'é¡ºç€', 'ç›¸ä¼¼', 'åšåˆ°', 'åŒæ ·', 'ä¸€é¢', 'å‰å', 'ä¹ˆ', 'å«', 'æ˜¯', 'æ¯”è¾ƒ', 'å•¥', 'å…·ä½“è¯´æ¥', 'æ€»ä¹‹', 'è®º', 'è¿™ä¹ˆäº›', 'ç›¸ç­‰', 'è¿™éº½', 'å•ªè¾¾', 'è®¾ä½¿', 'ç§»åŠ¨', 'æ€ä¹ˆæ ·', 'éšç€', 'ä½¿å¾—', 'åè¿‡æ¥', 'å„', 'å¯†åˆ‡', 'å¬å¼€', 'å“¼å”·', 'ä¸€æ—¦', 'æ€»çš„æ¥çœ‹', 'åªè¦', 'é›†ä¸­', 'å¤šæ¬¡', 'å½“ç€', 'å½»åº•', 'è¡Œä¸º', 'ä¸æ€•', 'è½¬è´´', 'å¼€å¤–', 'ä¸åŒ', 'å·¦å³', 'æœ‰åˆ©', 'ä¸€å¤©', 'å®£å¸ƒ', 'ä¸€è‡´', 'å¤šå°‘', 'ä¸¥æ ¼', 'å–”å”·', 'å„ç§', 'ç„¶åˆ™', 'çºµä½¿', 'ä¸€ä¸‹', 'ä¹Ÿ', 'å‡†å¤‡', 'ä¸”', 'å¦‚æ­¤', 'å´ä¸', 'çœ‹è§', 'è™½åˆ™', 'å±', 'è™½è¯´', 'å°±', 'é™„è¿‘', 'å’¦', 'é‚£äº›', 'å¥¹çš„', 'å¼•èµ·', 'å¿…ç„¶', 'ä¸€æ¥', 'ä¸å¯', 'é‡‡å–', 'æŸä¸ª', 'å“ªå¤©', 'ç”±', 'æ—äºº', 'ä½†æ˜¯', 'è®¤è¯†', 'ä¸ªäºº', 'å‘¸', 'å‡ºæ¥', 'å˜›', 'è”ç³»', 'è¦ä¸', 'å‡­', 'äº’ç›¸', 'æ—¢ç„¶', 'ç»´æŒ', 'è§„å®š', 'ä¸‡ä¸€', 'å› è€Œ', 'äº‰å–', 'ç€å‘¢', 'å…¶å®', 'æ€»ç»“', 'è®©', 'é€šè¿‡', 'ä¸ºä½•', 'ä¸æ¯”', 'å„è‡ª', 'å°¤å…¶', 'å‘¢', 'ä¸ä¹…', 'ä¸´', 'æœ›', 'é‚£ä¹ˆæ ·', 'ä¸€èˆ¬', 'é‚£æ ·', 'é‚£æ—¶', 'æ€éº½', 'é­åˆ°', 'çªç„¶', 'æœ€å¾Œ', 'çœ‹æ¥', 'ç„¶å', 'å“—', 'å…³äº', 'æˆ˜æ–—', 'ä»¥è‡³äº', 'è€Œå†µ', 'ä½•å¤„', 'æ„æˆ', 'å˜»', 'åˆ«è¯´', 'ä»¥å¾Œ', 'åŸæ¥', 'å“¼', 'æ€ä¹ˆåŠ', 'æ¥ç€', 'è¿‡å»', 'å…¶äºŒ', 'ä¸€èµ·', 'è·å¾—', 'æ— å®', 'å¦‚', 'å¿…è¦', 'æ˜¯çš„', 'å…·ä½“åœ°è¯´', 'æœ‰çš„', 'å¯¹äº', 'å–‚', 'çœ‹åˆ°', 'å“¦', 'ä»¥æ¥', 'å¦', 'å“‰', 'å°±æ˜¯è¯´', 'ä¸å¦‚', 'ç»§è€Œ', 'äºæ˜¯', 'ä¸¤è€…', 'å¦‚ä¸Šæ‰€è¿°', 'åæ¥', 'è®¤çœŸ', 'è¿åŒ', 'å…ˆå¾Œ', 'è‡³', 'å°±æ˜¯', 'è¾ƒä¹‹', 'åŒä¸€', 'ä¼¼ä¹', 'å¾…', 'é€ æˆ', 'è‡ªèº«', 'è‰¯å¥½', 'å¤šæ•°', 'å’³', 'ä¸€æ ·', 'å½“å‰', 'åŒæ–¹', 'é€æ¸', 'å³æˆ–', 'è€Œ', 'ç§¯æ', 'è‡³äº', 'å¦‚ä½•', 'å“©', 'ç›¸å¯¹', 'å—', 'åˆ°', 'åœ°', 'ç”±äº', 'éš', 'ä¸Šæ¥', 'ç»ƒä¹ ', 'ååº”', 'é‡è¦', 'è…¾', 'åº”è¯¥', 'å…¶ä½™', 'éå¸¸', 'ä¹Ÿæ˜¯', 'çªå‡º', 'å› ä¸º', 'ç„‰', 'å¤š', 'åŠè‡³', 'å¼ºçƒˆ', 'å„åœ°', 'æœ‰æ—¶', 'æˆ–æ˜¯', 'ä¼å›¾', 'ä¸€è¾¹', 'æŒ‰', 'æœ‰', 'ä¸­å°', 'ç‰¹åˆ«æ˜¯', 'å–', 'ä¹‹å', 'å‡ æ—¶', 'å”‰', 'ä¸¥é‡', 'å•', 'çœŸæ­£', 'å„ä½', 'å› æ­¤', 'çœ‹å‡º', 'å…¶ä»–', 'ä»¥è‡³', 'ä»Šå¤©', 'æ­£åœ¨', 'ä»äº‹', 'å“ªæ ·', 'å³è‹¥', 'å“ˆå“ˆ', 'ç›¸å¯¹è€Œè¨€', 'æœ‰åŠ›', 'å®ƒä»¬çš„', 'ä¸ºä¸»', 'å¥¹ä»¬', 'æœ‰äº›', 'ä¸€æ¬¡', 'ç›¸å½“', 'å¹¶ä¸æ˜¯', 'å½¼æ­¤', 'å—³', 'ä¸å¾—', 'æ—¢', 'ä¸Šå»', 'åŒ', 'æ€»çš„è¯´æ¥', 'ä¸ç®¡', 'æ— è®º', 'å…è®¸', 'ä¹ƒè‡³', 'ä¸ä»…', 'æœç„¶', 'ä¸º', 'å¯æ˜¯', 'ç”šè€Œ', 'ä¸è®º', 'è¿™ä¼šå„¿', 'ä¸‹å»', 'æ²¿', 'å—¡å—¡', 'å•Š', 'ä½œä¸º', 'ç…§', 'æ‰€æœ‰', 'æ²¡æœ‰', 'ä¸ºäº†', 'å‰è€…', 'ç»¼ä¸Šæ‰€è¿°', 'é™¤é', 'å¤§å¤šæ•°', 'æ˜¯å¦', 'æˆ‘çš„', 'æœ€é«˜', 'å°†', 'éœ€è¦', 'å¾€', 'è·Ÿ', 'è¿…é€Ÿ', 'æäº†', 'è¿›è€Œ', 'è‹¥æ˜¯', 'å˜ç™»', 'æ›¾ç»', 'å¤§å¤§', 'æœ¬ç€', 'å¤§å®¶', 'åˆç†', 'å¼€å§‹', 'ç›´æ¥', 'ç»§ç»­', 'æœ‰å…³', 'ä»è€Œ', 'èµ·è§', 'å‘ç€', 'æ ¹æœ¬', 'æ— æ³•', 'ç„¶å¾Œ', 'å½“ç„¶', 'æœ‰ç‚¹', 'æœ€è¿‘', 'ç›®å‰', 'å®æ„¿', 'å’±', 'å“Ÿ', 'è¿™ä¹ˆç‚¹å„¿', 'å¿…é¡»', 'ä»¥å', 'ä¿æŒ', 'ç…§ç€', 'å“', 'ä»¥ä¾¿', 'ä»»åŠ¡', 'è¯¸ä½', 'ä»', 'è°çŸ¥', 'çš„è¯', 'åˆ†åˆ«', 'æ»¡è¶³', 'ä¸è¿‡', 'å‰è¿›', 'å‘œå‘¼', 'å³ä»¤', 'æ‹¿', 'æ›¿', 'å‘ƒ', 'ä¹ƒ', 'å‘œ', 'ä»ç„¶', 'è‡ªå®¶', 'è€ƒè™‘', 'ä¸¾è¡Œ', 'å®ƒçš„', 'æŠŠ', 'èƒ½å¤Ÿ', 'ä¾ç…§', 'é‡åˆ°', 'å¥¹', 'å¯ä»¥', 'è½¬å˜', 'å¥½çš„', 'æ–¹é¢', 'èƒ½å¦', 'å°”å', 'å®¹æ˜“', 'åº”å½“', 'è¿ç”¨', 'æ˜¯ä¸æ˜¯', 'åŠ å…¥', 'å®é™…', 'æ—¶å€™', 'ç¦»', 'çœŸæ˜¯', 'ç»è¿‡', 'å“ªäº›', 'ä¸èƒ½', 'è¿‡æ¥', 'å€˜', 'ä½†', 'å˜˜', 'å¿ƒé‡Œ', 'ä¸æˆ', 'ä¹‹ä¸€', 'å®ç°', 'è‡ª', 'éšè‘—', 'å‘¨å›´', 'ä»¥å‰', 'ä»¥å…', 'ä¾é ', 'ç”šä¹ˆ', 'æ¯‹å®', 'ä¸ä½†', 'å½¼', 'è¿™äº›', 'å€˜æˆ–', 'å†µä¸”', 'æ­£å¦‚', 'è‡ªå·±', 'å·±', 'ä¸æ­¤åŒæ—¶', 'å•¦', 'å‡ ', 'å‡ºç°', 'å› ', 'ä¸€åˆ™', 'è¿˜æ˜¯', 'é€æ­¥', 'è€Œä¸”', 'å°‘æ•°', 'é‚£ä¸ª', 'æ¯å¹´', 'é˜¿', 'ä¸ç‰¹', 'å¦‚å…¶', 'è¶Šæ˜¯', 'ç›¸åŒ', 'åº”ç”¨', 'æˆ‘', 'ä¸‹é¢', 'æ·±å…¥', 'å‘•', 'éƒ¨åˆ†', 'ä¸€æ—¶', 'ä½•', 'åŠ', 'æ¸…æ¥š', 'è¦ä¸ç„¶', 'å…¶æ¬¡', 'æ€»è€Œè¨€ä¹‹', 'ä»¥ä¸Š', 'åªæœ‰', 'å¾Œé¢', 'å­˜åœ¨', 'è¦ä¸æ˜¯', 'ä¸å…¶', 'ä»–ä»¬', 'æ¼«è¯´', 'å‘', 'å˜¿', 'è¿™ç§', 'è®¸å¤š', 'è™½ç„¶', 'ä½•æ—¶', 'å“å‘€', 'åªé™', 'ä¹˜', 'å¾—', 'æ™®é', 'è¡¨ç¤º', 'è®¾è‹¥', 'å“ªå„¿', 'æ­¤å¤–', 'äº†', 'å“å“Ÿ', 'æ–¹ä¾¿', 'æ˜ç¡®', 'è¿™ä¸ª', 'å‘µ', 'æŠ‘æˆ–', 'ä¸æ•¢', 'å†è€…', 'å€˜ç„¶', 'å“ª', 'ä¾‹å¦‚', 'ä»»'}\n"
     ]
    }
   ],
   "source": [
    "# Get a list of stop words in English\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(f'Stop words in English:\\n{len(stop_words)}\\n{stop_words}')\n",
    "\n",
    "# Get a list of stop words in Chinese\n",
    "chi_stop_words = set(stopwords.words(\"chinese\"))\n",
    "print(f'Stop words in Chinese:\\n{len(chi_stop_words)}\\n{chi_stop_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ca914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print non-stop words\n",
    "def print_non_stop_words(s, stop_words, lang='en'):\n",
    "    print(f'\\n\\nOriginal:\\n {s}')\n",
    "    print(f'None-stop words:')\n",
    "    \n",
    "    if lang == 'en':\n",
    "        sentences = nltk.word_tokenize(s)\n",
    "    elif lang == 'zh':\n",
    "        # sentences = jieba.cut(s, cut_all=True)  # å…¨æ¨¡å¼ï¼Œè¿”å›è¿­ä»£å™¨\n",
    "        # sentences = jieba.cut(s, cut_all=False) # ç²¾ç¡®æ¨¡å¼ï¼Œè¿”å›è¿­ä»£å™¨\n",
    "        # sentences = jieba.lcut(s)  # ç²¾ç¡®æ¨¡å¼ï¼Œè¿”å›åˆ—è¡¨\n",
    "        sentences = jieba.cut_for_search(s)  # æœç´¢å¼•æ“æ¨¡å¼ï¼Œè¿”å›è¿­ä»£å™¨\n",
    "        # sentences = jieba.lcut_for_search(s)  # æœç´¢å¼•æ“æ¨¡å¼ï¼Œè¿”å›åˆ—è¡¨\n",
    "        print(type(sentences))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language. Use 'en' for English or 'zh' for Chinese.\")\n",
    "\n",
    "    for token in sentences:\n",
    "        if token not in stop_words:\n",
    "            print(token+' ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c18d4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = (\"Apple's name was inspired by Steve Jobs' visit. His visit was to an apple farm while on a fruitarian diet.\")\n",
    "chi_s = (\"è‹¹æœçš„åå­—æ¥æºäºå²è’‚å¤«Â·ä¹”å¸ƒæ–¯çš„è®¿é—®ã€‚ä»–åœ¨åƒæ°´æœçš„æ—¶å€™å‚è§‚äº†ä¸€ä¸ªè‹¹æœå†œåœºã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "fd61015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original:\n",
      " Apple's name was inspired by Steve Jobs' visit. His visit was to an apple farm while on a fruitarian diet.\n",
      "None-stop words:\n",
      "Apple 's name inspired Steve Jobs ' visit . His visit apple farm fruitarian diet . \n",
      "\n",
      "Original:\n",
      " è‹¹æœçš„åå­—æ¥æºäºå²è’‚å¤«Â·ä¹”å¸ƒæ–¯çš„è®¿é—®ã€‚ä»–åœ¨åƒæ°´æœçš„æ—¶å€™å‚è§‚äº†ä¸€ä¸ªè‹¹æœå†œåœºã€‚\n",
      "None-stop words:\n",
      "<class 'generator'>\n",
      "<class 'generator'>\n",
      "è‹¹æœ åå­— æ¥æº æºäº æ¥æºäº è’‚å¤« å²è’‚å¤« Â· ä¹”å¸ƒ å¸ƒæ–¯ ä¹”å¸ƒæ–¯ è®¿é—® ã€‚ åƒæ°´ æ°´æœ åƒæ°´æœ å‚è§‚ ä¸€ä¸ª è‹¹æœ å†œåœº ã€‚ "
     ]
    }
   ],
   "source": [
    "print_non_stop_words(s, stop_words)\n",
    "print_non_stop_words(chi_s, chi_stop_words, lang='zh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
