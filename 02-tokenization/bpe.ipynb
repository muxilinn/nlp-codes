{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Example\n",
    "\n",
    "This is an example of byte pair encoding from [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) by Sennrich et al. (2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab: dict[str, int]) -> dict[tuple[str, str], int]:\n",
    "    \"\"\"Get stats of token pairs.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair: tuple[str, str], v_in: dict[str, int]) -> dict[str, int]:\n",
    "    \"\"\"Merge a particular pair together and return the new vocabulary.\"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab={'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
      "top_pairs=[(('e', 's'), 9), (('s', 't'), 9), (('t', '</w>'), 9), (('w', 'e'), 8), (('l', 'o'), 7)]\n",
      "best=('e', 's'): 9\n",
      "vocab={'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "top_pairs=[(('es', 't'), 9), (('t', '</w>'), 9), (('l', 'o'), 7), (('o', 'w'), 7), (('n', 'e'), 6)]\n",
      "best=('es', 't'): 9\n",
      "vocab={'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "top_pairs=[(('est', '</w>'), 9), (('l', 'o'), 7), (('o', 'w'), 7), (('n', 'e'), 6), (('e', 'w'), 6)]\n",
      "best=('est', '</w>'): 9\n",
      "vocab={'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "top_pairs=[(('l', 'o'), 7), (('o', 'w'), 7), (('n', 'e'), 6), (('e', 'w'), 6), (('w', 'est</w>'), 6)]\n",
      "best=('l', 'o'): 7\n",
      "vocab={'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "top_pairs=[(('lo', 'w'), 7), (('n', 'e'), 6), (('e', 'w'), 6), (('w', 'est</w>'), 6), (('w', '</w>'), 5)]\n",
      "best=('lo', 'w'): 7\n",
      "vocab={'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "top_pairs=[(('n', 'e'), 6), (('e', 'w'), 6), (('w', 'est</w>'), 6), (('low', '</w>'), 5), (('w', 'i'), 3)]\n",
      "best=('n', 'e'): 6\n",
      "vocab={'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3}\n",
      "top_pairs=[(('ne', 'w'), 6), (('w', 'est</w>'), 6), (('low', '</w>'), 5), (('w', 'i'), 3), (('i', 'd'), 3)]\n",
      "best=('ne', 'w'): 6\n",
      "vocab={'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3}\n",
      "top_pairs=[(('new', 'est</w>'), 6), (('low', '</w>'), 5), (('w', 'i'), 3), (('i', 'd'), 3), (('d', 'est</w>'), 3)]\n",
      "best=('new', 'est</w>'): 6\n",
      "vocab={'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "top_pairs=[(('low', '</w>'), 5), (('w', 'i'), 3), (('i', 'd'), 3), (('d', 'est</w>'), 3), (('low', 'e'), 2)]\n",
      "best=('low', '</w>'): 5\n",
      "vocab={'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3}\n",
      "top_pairs=[(('w', 'i'), 3), (('i', 'd'), 3), (('d', 'est</w>'), 3), (('low', 'e'), 2), (('e', 'r'), 2)]\n",
      "best=('w', 'i'): 3\n"
     ]
    }
   ],
   "source": [
    "vocab = {\n",
    "    'l o w </w>' : 5,\n",
    "    'l o w e r </w>' : 2,\n",
    "    'n e w e s t </w>': 6,\n",
    "    'w i d e s t </w>': 3,\n",
    "}\n",
    "num_merges = 10\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(f\"{vocab=}\")\n",
    "    pairs = get_stats(vocab)\n",
    "    top_pairs = sorted(list(pairs.items()), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(f\"{top_pairs=}\")\n",
    "    best = top_pairs[0][0]\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f\"best={best}: {pairs[best]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentencepiece Encoding\n",
    "使用预训练模型的训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: ./tiansz/bert-base-chinese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 19:58:41,637 - modelscope - INFO - Got 7 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4502c7868bd487890919decddfd5faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 7 items:   0%|          | 0.00/7.00 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7689f23edf4114bf887898b6cdd4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebc837b7c2444d485199fe723ea4647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/392M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f78401d9aa14567acd35bad5081ec79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/2.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d05137a3134019914256f7046ef98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29815ff3fad4424be8acea5faa91672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f95b158df74b6bade3909500dfdef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd82fa7408d4223a0adabaeca2160fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.txt]:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 20:04:54,002 - modelscope - INFO - Download model 'tiansz/bert-base-chinese' successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已下载到：./tiansz/bert-base-chinese\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖（若未安装）\n",
    "# ！pip install modelscope transformers # 从魔搭社区安装transformers\n",
    "\n",
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "# 从魔搭社区下载 bert-base-chinese 模型\n",
    "model_dir = snapshot_download(\n",
    "    model_id=\"tiansz/bert-base-chinese\",  # 魔搭社区的模型ID（与Hugging Face兼容）\n",
    "    cache_dir=\"./\"  # 下载到当前目录（可自定义路径）\n",
    ")\n",
    "\n",
    "print(f\"模型已下载到：{model_dir}\")  # 输出：./bert-base-chinese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 'lowest' 分词结果：['low', '##est']\n",
      "中文句子分词结果：['当', '我', '还', '只', '有', '六', '岁', '的', '时', '候', '在', '一', '本', '描', '写', '原', '始', '森', '林', '的', '名', '叫', '真', '实', '的', '故', '事', '的', '书', '中', '看', '到', '了', '一', '幅', '精', '彩', '的', '插', '画']\n",
      "Chinese: ['japan', 'is', 'a', 'country', 'in', 'east', 'asia', '.']\n",
      "nltk: ['japan', 'is', 'a', 'country', 'in', 'east', 'asia', '.']\n",
      "english_text3: ['p', '##ne', '##um', '##on', '##ou', '##lt', '##ram', '##ic', '##ros', '##co', '##pi', '##cs', '##il', '##ico', '##vo', '##lc', '##ano', '##con', '##io', '##sis']\n",
      "english_text3: ['ha', '##pp', '##ine', '##ss']\n",
      "japaness: ['japan', '##ess']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./tiansz/bert-base-chinese\")  # 路径与下载路径一致\n",
    "import nltk\n",
    "# nltk.download('punkt')  # 下载punkt分词模型\n",
    "\n",
    "# 待分词的文本\n",
    "english_text = \"lowest\"\n",
    "chinese_text = \"当我还只有六岁的时候在一本描写原始森林的名叫真实的故事的书中看到了一幅精彩的插画\"\n",
    "\n",
    "# 分词（返回子词列表）\n",
    "en_tokens = tokenizer.tokenize(english_text)\n",
    "zh_tokens = tokenizer.tokenize(chinese_text)\n",
    "\n",
    "print(f\"英文 '{english_text}' 分词结果：{en_tokens}\")\n",
    "print(f\"中文句子分词结果：{zh_tokens}\")\n",
    "\n",
    "english_text2 = \"Japan is a country in East Asia.\"\n",
    "en_tokens2 = tokenizer.tokenize(english_text2.lower())\n",
    "\n",
    "print(f'Chinese: {en_tokens2}')\n",
    "print(f'nltk: {nltk.word_tokenize(english_text2.lower())}')  # 简单的空格分词对比\n",
    "\n",
    "english_text3 = 'Pneumonoultramicroscopicsilicovolcanoconiosis'.lower()\n",
    "en_tokens3 = tokenizer.tokenize(english_text3)\n",
    "print(f'english_text3: {en_tokens3}')\n",
    "\n",
    "english_text4 = 'Happiness'.lower()\n",
    "en_tokens4 = tokenizer.tokenize(english_text4)\n",
    "print(f'english_text3: {en_tokens4}')\n",
    "\n",
    "english_text5 = 'Japaness'.lower()\n",
    "en_tokens5 = tokenizer.tokenize(english_text5)\n",
    "print(f'{english_text5}: {en_tokens5}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
