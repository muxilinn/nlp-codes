{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e350a56",
   "metadata": {},
   "source": [
    "# 训练情感分类器\n",
    "\n",
    "这是[深圳技术大学2025秋季微专业课](https://hqyang.github.io/nlp-codes/)使用的Jupyter Notebook。下面尝试基于数据进行训练。具体来说，它使用词袋提取特征，并使用多项式逻辑回归算法训练分类器。\n",
    "\n",
    "它将接收文本`X`并返回一个`标签`，如果文本的情感类型是积极的，则为“1”，如果文本的情感类型是消极的，则为“-1”，如果文本的情感类型是中性的，则为“0”。\n",
    "你可以在[Stanford Sentiment Treebank](http://nlp.stanford.edu/sentiment/index.html)运行此脚本测试你的分类器的准确性。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3990d294",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8b81b",
   "metadata": {},
   "source": [
    "# Implement MultinomialLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0350d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, batch_size=32, learning_rate=0.01, reg_param=0.01, epochs=100):\n",
    "        \"\"\"\n",
    "        初始化多项式逻辑回归模型（多分类）\n",
    "        :param batch_size: 批处理大小\n",
    "        :param learning_rate: 学习率\n",
    "        :param reg_param: L2正则化参数\n",
    "        :param epochs: 训练轮数\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.epochs = epochs\n",
    "        self.vocab = None  # 词汇表（词→索引）\n",
    "        self.classes = None  # 类别列表\n",
    "        self.W = None  # 权重矩阵 (特征数 + 1, 类别数)，+1为偏置项\n",
    "        self.train_losses = []  # 记录训练损失\n",
    "        self.train_accs = []  # 记录训练准确率\n",
    "        self.val_losses = []  # 记录验证损失\n",
    "        self.val_accs = []  # 记录验证准确率\n",
    "\n",
    "    def _extract_features(self, text):\n",
    "        \"\"\"提取单个文本的词频特征\"\"\"\n",
    "        features = defaultdict(int)\n",
    "        for word in text.split():\n",
    "            features[word] += 1\n",
    "\n",
    "        # Implement your feature extraction logic here\n",
    "        # 建议: 简单的词袋模型：按空格分词并计数\n",
    "        pass\n",
    "    \n",
    "        return features\n",
    "\n",
    "    def _build_vocab(self, texts):\n",
    "        \"\"\"从训练文本构建词汇表\"\"\"\n",
    "        vocab = {}\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "\n",
    "        # Implement your vocabulary building logic here\n",
    "        # 建议: 遍历所有文本，按出现顺序为每个唯一词分配索引\n",
    "        pass\n",
    "\n",
    "        return vocab\n",
    "\n",
    "    def _texts_to_matrix(self, texts):\n",
    "        \"\"\"将文本列表转换为词频矩阵（添加偏置项）\"\"\"\n",
    "        if not self.vocab:\n",
    "            raise ValueError(\"词汇表未初始化，请先训练模型\")\n",
    "        \n",
    "        n_samples = len(texts)\n",
    "        n_features = len(self.vocab)\n",
    "        # 初始化矩阵（+1用于偏置项）\n",
    "        X = np.zeros((n_samples, n_features + 1))\n",
    "        X[:, 0] = 1.0  # 第0列固定为1（偏置项）\n",
    "\n",
    "        # Implement your text to matrix conversion logic here\n",
    "        # 建议: 对每个文本，提取词频特征并填充矩阵\n",
    "        pass        \n",
    "\n",
    "        return X\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"稳定版softmax函数（避免数值溢出）\"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"计算交叉熵损失\"\"\"\n",
    "        n_samples = y_pred.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(n_samples), y_true] + 1e-15)\n",
    "        loss = np.sum(log_likelihood) / n_samples\n",
    "        # 添加L2正则化项\n",
    "        loss += (self.reg_param / 2) * np.sum(self.W[1:, :] ** 2)  # 不正则化偏置项\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X_train_text, y_train, X_val_text=None, y_val=None):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param X_train_text: 训练文本列表\n",
    "        :param y_train: 训练标签列表\n",
    "        :param X_val_text: 验证文本列表（可选）\n",
    "        :param y_val: 验证标签列表（可选）\n",
    "        \"\"\"\n",
    "        # 1. 构建词汇表和类别映射\n",
    "        self.vocab = self._build_vocab(X_train_text)\n",
    "        self.classes = np.unique(y_train)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = len(self.vocab)\n",
    "        \n",
    "        # 2. 转换文本为特征矩阵（含偏置项）\n",
    "        X_train = self._texts_to_matrix(X_train_text)\n",
    "        # 将标签映射为0~n_classes-1（便于one-hot处理）\n",
    "        y_train_mapped = np.array([np.where(self.classes == y)[0][0] for y in y_train])\n",
    "        \n",
    "        # 3. 初始化权重矩阵（随机初始化）\n",
    "        self.W = np.random.randn(n_features + 1, n_classes) * 0.01  # +1是偏置项\n",
    "        \n",
    "        # 4. 小批量梯度下降训练\n",
    "        n_samples = X_train.shape[0]\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Training\"):\n",
    "            # 打乱训练数据\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train_mapped[indices]\n",
    "            \n",
    "            # 按批次更新\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "                \n",
    "                # 前向传播：计算预测概率\n",
    "                # Implement your forward propagation logic here\n",
    "                z = X_batch @ self.W\n",
    "                y_pred = self._softmax(z)\n",
    "                # 计算梯度（含正则化）\n",
    "                # Implement your gradient computation logic here\n",
    "                n_batch = X_batch.shape[0]\n",
    "                y_one_hot = np.zeros_like(y_pred)\n",
    "                y_one_hot[np.arange(n_batch), y_batch] = 1\n",
    "                gradient = (X_batch.T @ (y_pred - y_one_hot)) / n_batch\n",
    "                # 添加正则化梯度（不正则化偏置项）\n",
    "                gradient[1:, :] += self.reg_param * self.W[1:, :]\n",
    "                \n",
    "                # 更新权重\n",
    "                # Implement your weight update logic here\n",
    "                self.W -= self.learning_rate * gradient\n",
    "            \n",
    "            # 记录当前epoch的损失和准确率\n",
    "            # 训练集指标\n",
    "            z_train = X_train @ self.W\n",
    "            y_pred_train = self._softmax(z_train)\n",
    "            train_loss = self._cross_entropy_loss(y_pred_train, y_train_mapped)\n",
    "            train_acc = np.mean(np.argmax(y_pred_train, axis=1) == y_train_mapped)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accs.append(train_acc)\n",
    "            \n",
    "            # 验证集指标（如果提供）\n",
    "            if X_val_text is not None and y_val is not None:\n",
    "                X_val = self._texts_to_matrix(X_val_text)\n",
    "                y_val_mapped = np.array([np.where(self.classes == y)[0][0] for y in y_val])\n",
    "                z_val = X_val @ self.W\n",
    "                y_pred_val = self._softmax(z_val)\n",
    "                val_loss = self._cross_entropy_loss(y_pred_val, y_val_mapped)\n",
    "                val_acc = np.mean(np.argmax(y_pred_val, axis=1) == y_val_mapped)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accs.append(val_acc)\n",
    "            \n",
    "            # 每10轮打印一次进度\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                log = f\"Epoch {epoch+1}/{self.epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\"\n",
    "                if X_val_text is not None:\n",
    "                    log += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "                print(log)\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"预测文本类别\"\"\"\n",
    "        X = self._texts_to_matrix(texts)\n",
    "        z = X @ self.W\n",
    "        y_pred_mapped = np.argmax(self._softmax(z), axis=1)\n",
    "        return self.classes[y_pred_mapped]\n",
    "\n",
    "    def score(self, texts, labels):\n",
    "        \"\"\"计算准确率\"\"\"\n",
    "        y_pred = self.predict(texts)\n",
    "        return np.mean(y_pred == labels)\n",
    "\n",
    "    def plot_convergence(self):\n",
    "        \"\"\"绘制训练/验证的损失和准确率收敛曲线\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # 损失曲线\n",
    "        ax1.plot(range(1, self.epochs + 1), self.train_losses, label=\"Train Loss\")\n",
    "        if self.val_losses:\n",
    "            ax1.plot(range(1, self.epochs + 1), self.val_losses, label=\"Val Loss\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Loss Convergence\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # 准确率曲线\n",
    "        ax2.plot(range(1, self.epochs + 1), self.train_accs, label=\"Train Accuracy\")\n",
    "        if self.val_accs:\n",
    "            ax2.plot(range(1, self.epochs + 1), self.val_accs, label=\"Val Accuracy\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Accuracy Convergence\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据读取函数（复用之前的逻辑）\n",
    "def read_xy_data(filename):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            label, text = line.split(' ||| ')\n",
    "            x_data.append(text)\n",
    "            y_data.append(int(label))\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed587e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 8544\n",
      "验证集样本数: 1101\n",
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 2. 加载数据\n",
    "x_train, y_train = read_xy_data('../data/sst-sentiment-text-threeclass/train.txt')\n",
    "x_dev, y_dev = read_xy_data('../data/sst-sentiment-text-threeclass/dev.txt')\n",
    "\n",
    "print(f\"训练集样本数: {len(x_train)}\")\n",
    "print(f\"验证集样本数: {len(x_dev)}\")\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4de4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# 初始化并训练模型\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m MultinomialLogisticRegression(\n\u001b[1;32m      5\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      6\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m      7\u001b[0m         reg_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m      8\u001b[0m         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# 4. 评估模型\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(x_train, y_train)\n",
      "Cell \u001b[0;32mIn[10], line 111\u001b[0m, in \u001b[0;36mMultinomialLogisticRegression.fit\u001b[0;34m(self, X_train_text, y_train, X_val_text, y_val)\u001b[0m\n\u001b[1;32m    109\u001b[0m n_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    110\u001b[0m y_onehot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_pred)\n\u001b[0;32m--> 111\u001b[0m y_onehot[np\u001b[38;5;241m.\u001b[39marange(n_batch), y_batch] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    112\u001b[0m grad \u001b[38;5;241m=\u001b[39m (X_batch\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (y_pred \u001b[38;5;241m-\u001b[39m y_onehot)) \u001b[38;5;241m/\u001b[39m n_batch  \u001b[38;5;66;03m# 损失梯度\u001b[39;00m\n\u001b[1;32m    113\u001b[0m grad[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_param \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# 正则化梯度（排除偏置）\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# 示例：情感分类任务测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化并训练模型\n",
    "    model = MultinomialLogisticRegression(\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "        reg_param=0.1,\n",
    "        epochs=50\n",
    "    )\n",
    "    model.fit(x_train, y_train, X_val_text=x_dev, y_val=y_dev)\n",
    "\n",
    "    # 4. 评估模型\n",
    "    train_acc = model.score(x_train, y_train)\n",
    "    dev_acc = model.score(x_dev, y_dev)\n",
    "    print(f\"\\nTrain Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Dev/Test Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "    # 5. 绘制收敛曲线\n",
    "    model.plot_convergence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-fall25-assign1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
