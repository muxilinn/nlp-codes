{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e350a56",
   "metadata": {},
   "source": [
    "# 训练情感分类器\n",
    "\n",
    "这是[深圳技术大学2025秋季微专业课](https://hqyang.github.io/nlp-codes/)使用的Jupyter Notebook。下面尝试基于数据进行训练。具体来说，它使用词袋提取特征，并使用多项式逻辑回归算法训练分类器。\n",
    "\n",
    "它将接收文本`X`并返回一个`标签`，如果文本的情感类型是积极的，则为“1”，如果文本的情感类型是消极的，则为“-1”，如果文本的情感类型是中性的，则为“0”。\n",
    "你可以在[Stanford Sentiment Treebank](http://nlp.stanford.edu/sentiment/index.html)运行此脚本测试你的分类器的准确性。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8b81b",
   "metadata": {},
   "source": [
    "# Implement MultinomialLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0350d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, batch_size=32, learning_rate=0.01, reg_param=0.01, epochs=100):\n",
    "        \"\"\"\n",
    "        初始化多项式逻辑回归模型（多分类）\n",
    "        :param batch_size: 批处理大小\n",
    "        :param learning_rate: 学习率\n",
    "        :param reg_param: L2正则化参数\n",
    "        :param epochs: 训练轮数\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.epochs = epochs\n",
    "        self.vocab = None  # 词汇表（词→索引）\n",
    "        self.classes = None  # 类别列表\n",
    "        self.W = None  # 权重矩阵 (特征数 + 1, 类别数)，+1为偏置项\n",
    "        self.train_losses = []  # 记录训练损失\n",
    "        self.train_accs = []  # 记录训练准确率\n",
    "        self.val_losses = []  # 记录验证损失\n",
    "        self.val_accs = []  # 记录验证准确率\n",
    "\n",
    "    def _extract_features(self, text):\n",
    "        \"\"\"提取单个文本的词频特征\"\"\"\n",
    "        features = defaultdict(int)\n",
    "        \n",
    "        # Implement your feature extraction logic here\n",
    "        # 建议: 简单的词袋模型：按空格分词并计数\n",
    "        \n",
    "        return features\n",
    "\n",
    "    def _build_vocab(self, texts):\n",
    "        \"\"\"从训练文本构建词汇表\"\"\"\n",
    "        vocab = {}\n",
    "        idx = 0\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = idx\n",
    "                    idx += 1\n",
    "        return vocab\n",
    "\n",
    "    def _texts_to_matrix(self, texts):\n",
    "        \"\"\"将文本列表转换为词频矩阵（添加偏置项）\"\"\"\n",
    "        if not self.vocab:\n",
    "            raise ValueError(\"词汇表未初始化，请先训练模型\")\n",
    "        \n",
    "        n_samples = len(texts)\n",
    "        n_features = len(self.vocab)\n",
    "        # 初始化矩阵（+1用于偏置项）\n",
    "        X = np.zeros((n_samples, n_features + 1))\n",
    "        X[:, 0] = 1.0  # 第0列固定为1（偏置项）\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            features = self._extract_features(text)\n",
    "            for word, count in features.items():\n",
    "                if word in self.vocab:\n",
    "                    col_idx = self.vocab[word] + 1  # +1是因为第0列是偏置\n",
    "                    X[i, col_idx] = count\n",
    "        return X\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"稳定版softmax函数（避免数值溢出）\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"计算交叉熵损失\"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_train_text, y_train, X_val_text=None, y_val=None):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param X_train_text: 训练文本列表\n",
    "        :param y_train: 训练标签列表\n",
    "        :param X_val_text: 验证文本列表（可选）\n",
    "        :param y_val: 验证标签列表（可选）\n",
    "        \"\"\"\n",
    "        # 1. 构建词汇表和类别映射\n",
    "        self.vocab = self._build_vocab(X_train_text)\n",
    "        self.classes = np.unique(y_train)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = len(self.vocab)\n",
    "        \n",
    "        # 2. 转换文本为特征矩阵（含偏置项）\n",
    "        X_train = self._texts_to_matrix(X_train_text)\n",
    "        # 将标签映射为0~n_classes-1（便于one-hot处理）\n",
    "        y_train_mapped = np.array([np.where(self.classes == y)[0][0] for y in y_train])\n",
    "        \n",
    "        # 3. 初始化权重矩阵（随机初始化）\n",
    "        self.W = np.random.randn(n_features + 1, n_classes) * 0.01  # +1是偏置项\n",
    "        \n",
    "        # 4. 小批量梯度下降训练\n",
    "        n_samples = X_train.shape[0]\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Training\"):\n",
    "            # 打乱训练数据\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train_mapped[indices]\n",
    "            \n",
    "            # 按批次更新\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "                \n",
    "                # 前向传播：计算预测概率\n",
    "                z = X_batch @ self.W  # (batch_size, n_classes)\n",
    "                y_pred = self._softmax(z)\n",
    "                \n",
    "                # 计算梯度（含正则化）\n",
    "                n_batch = X_batch.shape[0]\n",
    "                y_onehot = np.zeros_like(y_pred)\n",
    "                y_onehot[np.arange(n_batch), y_batch] = 1.0\n",
    "                grad = (X_batch.T @ (y_pred - y_onehot)) / n_batch  # 损失梯度\n",
    "                grad[1:] += self.reg_param * self.W[1:]  # 正则化梯度（排除偏置）\n",
    "                \n",
    "                # 更新权重\n",
    "                self.W -= self.learning_rate * grad\n",
    "            \n",
    "            # 记录当前epoch的损失和准确率\n",
    "            # 训练集指标\n",
    "            z_train = X_train @ self.W\n",
    "            y_pred_train = self._softmax(z_train)\n",
    "            train_loss = self._cross_entropy_loss(y_pred_train, y_train_mapped)\n",
    "            train_acc = np.mean(np.argmax(y_pred_train, axis=1) == y_train_mapped)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accs.append(train_acc)\n",
    "            \n",
    "            # 验证集指标（如果提供）\n",
    "            if X_val_text is not None and y_val is not None:\n",
    "                X_val = self._texts_to_matrix(X_val_text)\n",
    "                y_val_mapped = np.array([np.where(self.classes == y)[0][0] for y in y_val])\n",
    "                z_val = X_val @ self.W\n",
    "                y_pred_val = self._softmax(z_val)\n",
    "                val_loss = self._cross_entropy_loss(y_pred_val, y_val_mapped)\n",
    "                val_acc = np.mean(np.argmax(y_pred_val, axis=1) == y_val_mapped)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accs.append(val_acc)\n",
    "            \n",
    "            # 每10轮打印一次进度\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                log = f\"Epoch {epoch+1}/{self.epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\"\n",
    "                if X_val_text is not None:\n",
    "                    log += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "                print(log)\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"预测文本类别\"\"\"\n",
    "        X = self._texts_to_matrix(texts)\n",
    "        z = X @ self.W\n",
    "        y_pred_mapped = np.argmax(self._softmax(z), axis=1)\n",
    "        return self.classes[y_pred_mapped]\n",
    "\n",
    "    def score(self, texts, labels):\n",
    "        \"\"\"计算准确率\"\"\"\n",
    "        y_pred = self.predict(texts)\n",
    "        return np.mean(y_pred == labels)\n",
    "\n",
    "    def plot_convergence(self):\n",
    "        \"\"\"绘制训练/验证的损失和准确率收敛曲线\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # 损失曲线\n",
    "        ax1.plot(range(1, self.epochs + 1), self.train_losses, label=\"Train Loss\")\n",
    "        if self.val_losses:\n",
    "            ax1.plot(range(1, self.epochs + 1), self.val_losses, label=\"Val Loss\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Loss Convergence\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # 准确率曲线\n",
    "        ax2.plot(range(1, self.epochs + 1), self.train_accs, label=\"Train Accuracy\")\n",
    "        if self.val_accs:\n",
    "            ax2.plot(range(1, self.epochs + 1), self.val_accs, label=\"Val Accuracy\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Accuracy Convergence\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddbbc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据读取函数（复用之前的逻辑）\n",
    "def read_xy_data(filename):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            label, text = line.split(' ||| ')\n",
    "            x_data.append(text)\n",
    "            y_data.append(int(label))\n",
    "    return x_data, y_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
