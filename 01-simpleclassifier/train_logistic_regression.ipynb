{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e728e06",
   "metadata": {},
   "source": [
    "# 包加载 Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b621b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab3a243",
   "metadata": {},
   "source": [
    "# Implement MultinomialLogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bb1789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialLogisticRegression:\n",
    "    def __init__(self, batch_size=32, learning_rate=0.01, reg_param=0.01, epochs=100):\n",
    "        \"\"\"\n",
    "        初始化多项式逻辑回归模型（多分类）\n",
    "        :param batch_size: 批处理大小\n",
    "        :param learning_rate: 学习率\n",
    "        :param reg_param: L2正则化参数\n",
    "        :param epochs: 训练轮数\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_param = reg_param\n",
    "        self.epochs = epochs\n",
    "        self.vocab = None  # 词汇表（词→索引）\n",
    "        self.classes = None  # 类别列表\n",
    "        self.W = None  # 权重矩阵 (特征数 + 1, 类别数)，+1为偏置项\n",
    "        self.train_losses = []  # 记录训练损失\n",
    "        self.train_accs = []  # 记录训练准确率\n",
    "        self.val_losses = []  # 记录验证损失\n",
    "        self.val_accs = []  # 记录验证准确率\n",
    "\n",
    "    def _extract_features(self, text):\n",
    "        \"\"\"提取单个文本的词频特征\"\"\"\n",
    "        features = defaultdict(int)\n",
    "        for word in text.split():\n",
    "            features[word] += 1\n",
    "        return features\n",
    "\n",
    "    def _build_vocab(self, texts):\n",
    "        \"\"\"从训练文本构建词汇表\"\"\"\n",
    "        vocab = {}\n",
    "        idx = 0\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = idx\n",
    "                    idx += 1\n",
    "        return vocab\n",
    "\n",
    "    def _texts_to_matrix(self, texts):\n",
    "        \"\"\"将文本列表转换为词频矩阵（添加偏置项）\"\"\"\n",
    "        if not self.vocab:\n",
    "            raise ValueError(\"词汇表未初始化，请先训练模型\")\n",
    "        \n",
    "        n_samples = len(texts)\n",
    "        n_features = len(self.vocab)\n",
    "        # 初始化矩阵（+1用于偏置项）\n",
    "        X = np.zeros((n_samples, n_features + 1))\n",
    "        X[:, 0] = 1.0  # 第0列固定为1（偏置项）\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            features = self._extract_features(text)\n",
    "            for word, count in features.items():\n",
    "                if word in self.vocab:\n",
    "                    col_idx = self.vocab[word] + 1  # +1是因为第0列是偏置\n",
    "                    X[i, col_idx] = count\n",
    "        return X\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"稳定版softmax函数（避免数值溢出）\"\"\"\n",
    "        z_max = np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z - z_max)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _cross_entropy_loss(self, y_pred, y_true):\n",
    "        \"\"\"计算交叉熵损失（含L2正则化）\"\"\"\n",
    "        n_samples = y_pred.shape[0]\n",
    "        # 真实标签的one-hot编码\n",
    "        y_onehot = np.zeros_like(y_pred)\n",
    "        y_onehot[np.arange(n_samples), y_true] = 1.0\n",
    "        # 交叉熵损失（加epsilon避免log(0)）\n",
    "        loss = -np.mean(np.sum(y_onehot * np.log(y_pred + 1e-10), axis=1))\n",
    "        # 添加L2正则化（排除偏置项的正则化）\n",
    "        l2_loss = 0.5 * self.reg_param * np.sum(self.W[1:] ** 2)  # W[0]是偏置，不正则化\n",
    "        return loss + l2_loss\n",
    "\n",
    "    def fit(self, X_train_text, y_train, X_val_text=None, y_val=None):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        :param X_train_text: 训练文本列表\n",
    "        :param y_train: 训练标签列表\n",
    "        :param X_val_text: 验证文本列表（可选）\n",
    "        :param y_val: 验证标签列表（可选）\n",
    "        \"\"\"\n",
    "        # 1. 构建词汇表和类别映射\n",
    "        self.vocab = self._build_vocab(X_train_text)\n",
    "        self.classes = np.unique(y_train)\n",
    "        n_classes = len(self.classes)\n",
    "        n_features = len(self.vocab)\n",
    "        \n",
    "        # 2. 转换文本为特征矩阵（含偏置项）\n",
    "        X_train = self._texts_to_matrix(X_train_text)\n",
    "        # 将标签映射为0~n_classes-1（便于one-hot处理）\n",
    "        y_train_mapped = np.array([np.where(self.classes == y)[0][0] for y in y_train])\n",
    "        \n",
    "        # 3. 初始化权重矩阵（随机初始化）\n",
    "        self.W = np.random.randn(n_features + 1, n_classes) * 0.01  # +1是偏置项\n",
    "        \n",
    "        # 4. 小批量梯度下降训练\n",
    "        n_samples = X_train.shape[0]\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Training\"):\n",
    "            # 打乱训练数据\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train_mapped[indices]\n",
    "            \n",
    "            # 按批次更新\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "                \n",
    "                # 前向传播：计算预测概率\n",
    "                z = X_batch @ self.W  # (batch_size, n_classes)\n",
    "                y_pred = self._softmax(z)\n",
    "                \n",
    "                # 计算梯度（含正则化）\n",
    "                n_batch = X_batch.shape[0]\n",
    "                y_onehot = np.zeros_like(y_pred)\n",
    "                y_onehot[np.arange(n_batch), y_batch] = 1.0\n",
    "                grad = (X_batch.T @ (y_pred - y_onehot)) / n_batch  # 损失梯度\n",
    "                grad[1:] += self.reg_param * self.W[1:]  # 正则化梯度（排除偏置）\n",
    "                \n",
    "                # 更新权重\n",
    "                self.W -= self.learning_rate * grad\n",
    "            \n",
    "            # 记录当前epoch的损失和准确率\n",
    "            # 训练集指标\n",
    "            z_train = X_train @ self.W\n",
    "            y_pred_train = self._softmax(z_train)\n",
    "            train_loss = self._cross_entropy_loss(y_pred_train, y_train_mapped)\n",
    "            train_acc = np.mean(np.argmax(y_pred_train, axis=1) == y_train_mapped)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accs.append(train_acc)\n",
    "            \n",
    "            # 验证集指标（如果提供）\n",
    "            if X_val_text is not None and y_val is not None:\n",
    "                X_val = self._texts_to_matrix(X_val_text)\n",
    "                y_val_mapped = np.array([np.where(self.classes == y)[0][0] for y in y_val])\n",
    "                z_val = X_val @ self.W\n",
    "                y_pred_val = self._softmax(z_val)\n",
    "                val_loss = self._cross_entropy_loss(y_pred_val, y_val_mapped)\n",
    "                val_acc = np.mean(np.argmax(y_pred_val, axis=1) == y_val_mapped)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accs.append(val_acc)\n",
    "            \n",
    "            # 每10轮打印一次进度\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                log = f\"Epoch {epoch+1}/{self.epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\"\n",
    "                if X_val_text is not None:\n",
    "                    log += f\" | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "                print(log)\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"预测文本类别\"\"\"\n",
    "        X = self._texts_to_matrix(texts)\n",
    "        z = X @ self.W\n",
    "        y_pred_mapped = np.argmax(self._softmax(z), axis=1)\n",
    "        return self.classes[y_pred_mapped]\n",
    "\n",
    "    def score(self, texts, labels):\n",
    "        \"\"\"计算准确率\"\"\"\n",
    "        y_pred = self.predict(texts)\n",
    "        return np.mean(y_pred == labels)\n",
    "\n",
    "    def plot_convergence(self):\n",
    "        \"\"\"绘制训练/验证的损失和准确率收敛曲线\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # 损失曲线\n",
    "        ax1.plot(range(1, self.epochs + 1), self.train_losses, label=\"Train Loss\")\n",
    "        if self.val_losses:\n",
    "            ax1.plot(range(1, self.epochs + 1), self.val_losses, label=\"Val Loss\")\n",
    "        ax1.set_xlabel(\"Epochs\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Loss Convergence\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # 准确率曲线\n",
    "        ax2.plot(range(1, self.epochs + 1), self.train_accs, label=\"Train Accuracy\")\n",
    "        if self.val_accs:\n",
    "            ax2.plot(range(1, self.epochs + 1), self.val_accs, label=\"Val Accuracy\")\n",
    "        ax2.set_xlabel(\"Epochs\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Accuracy Convergence\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f22520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据读取函数（复用之前的逻辑）\n",
    "def read_xy_data(filename):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            label, text = line.split(' ||| ')\n",
    "            x_data.append(text)\n",
    "            y_data.append(int(label))\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ce161",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/sst-sentiment-text-threeclass/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. 加载数据\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mread_xy_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/sst-sentiment-text-threeclass/train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m x_dev, y_dev \u001b[38;5;241m=\u001b[39m read_xy_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/sst-sentiment-text-threeclass/dev.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mread_xy_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m x_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m y_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m         line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/sst-sentiment-text-threeclass/train.txt'"
     ]
    }
   ],
   "source": [
    "# 2. 加载数据\n",
    "x_train, y_train = read_xy_data('../data/sst-sentiment-text-threeclass/train.txt')\n",
    "x_dev, y_dev = read_xy_data('../data/sst-sentiment-text-threeclass/dev.txt')\n",
    "x_train, y_train = read_xy_data('../data/sst-sentiment-text-threeclass/train.txt')\n",
    "x_dev, y_dev = read_xy_data('../data/sst-sentiment-text-threeclass/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b924e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：情感分类任务测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 3. 初始化并训练模型\n",
    "    model = MultinomialLogisticRegression(\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "        reg_param=0.1,\n",
    "        epochs=50\n",
    "    )\n",
    "    model.fit(x_train, y_train, x_dev, y_dev)\n",
    "\n",
    "    # 4. 评估模型\n",
    "    train_acc = model.score(x_train, y_train)\n",
    "    dev_acc = model.score(x_dev, y_dev)\n",
    "    print(f\"\\nFinal Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Final Dev Accuracy: {dev_acc:.4f}\")\n",
    "\n",
    "    # 5. 绘制收敛曲线\n",
    "    model.plot_convergence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
